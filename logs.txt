* 
* ==> Audit <==
* |---------|----------------|----------|--------|---------|---------------------|---------------------|
| Command |      Args      | Profile  |  User  | Version |     Start Time      |      End Time       |
|---------|----------------|----------|--------|---------|---------------------|---------------------|
| start   |                | minikube | tapiwa | v1.31.1 | 13 Aug 23 22:36 CDT | 13 Aug 23 22:39 CDT |
| kubectl | -- get pods -A | minikube | tapiwa | v1.31.1 | 13 Aug 23 22:43 CDT | 13 Aug 23 22:43 CDT |
| kubectl | -- get pods    | minikube | tapiwa | v1.31.1 | 13 Aug 23 22:43 CDT | 13 Aug 23 22:43 CDT |
| start   |                | minikube | tapiwa | v1.31.1 | 20 Aug 23 01:50 CDT | 20 Aug 23 01:52 CDT |
| service | chatbot        | minikube | tapiwa | v1.31.1 | 20 Aug 23 02:43 CDT |                     |
|---------|----------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/08/20 01:50:34
Running on machine: DESKTOP-PRNJG9L
Binary: Built with gc go1.20.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0820 01:50:34.783521    1611 out.go:296] Setting OutFile to fd 1 ...
I0820 01:50:34.785221    1611 out.go:348] isatty.IsTerminal(1) = true
I0820 01:50:34.785233    1611 out.go:309] Setting ErrFile to fd 2...
I0820 01:50:34.785246    1611 out.go:348] isatty.IsTerminal(2) = true
I0820 01:50:34.785653    1611 root.go:338] Updating PATH: /home/tapiwa/.minikube/bin
W0820 01:50:34.785976    1611 root.go:314] Error reading config file at /home/tapiwa/.minikube/config/config.json: open /home/tapiwa/.minikube/config/config.json: no such file or directory
I0820 01:50:34.799571    1611 out.go:303] Setting JSON to false
I0820 01:50:34.821102    1611 start.go:128] hostinfo: {"hostname":"DESKTOP-PRNJG9L","uptime":2919,"bootTime":1692511316,"procs":42,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.90.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"5acafb90-c1c4-470a-b8ce-90a17deef990"}
I0820 01:50:34.821272    1611 start.go:138] virtualization:  guest
I0820 01:50:34.876076    1611 out.go:177] üòÑ  minikube v1.31.1 on Ubuntu 22.04 (amd64)
I0820 01:50:34.987612    1611 notify.go:220] Checking for updates...
I0820 01:50:34.987798    1611 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0820 01:50:35.018430    1611 driver.go:373] Setting default libvirt URI to qemu:///system
I0820 01:50:35.320996    1611 docker.go:121] docker version: linux-24.0.5:Docker Engine - Community
I0820 01:50:35.321076    1611 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0820 01:50:35.402172    1611 lock.go:35] WriteFile acquiring /home/tapiwa/.minikube/last_update_check: {Name:mkb721c7fdb8095ab165ed8aef683c4b9704d4b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0820 01:50:35.475413    1611 out.go:177] üéâ  minikube 1.31.2 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.31.2
I0820 01:50:35.527823    1611 out.go:177] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0820 01:50:38.708448    1611 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.387347937s)
I0820 01:50:38.708964    1611 info.go:266] docker info: {ID:2f888634-bd8d-4a63-829c-d6e2d45de3c8 Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:29 OomKillDisable:true NGoroutines:43 SystemTime:2023-08-20 01:50:38.700222101 -0500 CDT LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8269004800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-PRNJG9L Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2]] Warnings:<nil>}}
I0820 01:50:38.709086    1611 docker.go:294] overlay module found
I0820 01:50:38.763229    1611 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0820 01:50:38.824925    1611 start.go:298] selected driver: docker
I0820 01:50:38.824946    1611 start.go:898] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/tapiwa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0820 01:50:38.825063    1611 start.go:909] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0820 01:50:38.825160    1611 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0820 01:50:39.185055    1611 info.go:266] docker info: {ID:2f888634-bd8d-4a63-829c-d6e2d45de3c8 Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:29 OomKillDisable:true NGoroutines:43 SystemTime:2023-08-20 01:50:39.17590055 -0500 CDT LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8269004800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-PRNJG9L Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2]] Warnings:<nil>}}
I0820 01:50:39.185610    1611 cni.go:84] Creating CNI manager for ""
I0820 01:50:39.185622    1611 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0820 01:50:39.185630    1611 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/tapiwa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0820 01:50:39.255620    1611 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0820 01:50:39.316050    1611 cache.go:122] Beginning downloading kic base image for docker with docker
I0820 01:50:39.372361    1611 out.go:177] üöú  Pulling base image ...
I0820 01:50:39.427721    1611 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0820 01:50:39.427768    1611 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0820 01:50:39.427885    1611 preload.go:148] Found local preload: /home/tapiwa/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4
I0820 01:50:39.427893    1611 cache.go:57] Caching tarball of preloaded images
I0820 01:50:39.428030    1611 preload.go:174] Found /home/tapiwa/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0820 01:50:39.428040    1611 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.3 on docker
I0820 01:50:39.428148    1611 profile.go:148] Saving config to /home/tapiwa/.minikube/profiles/minikube/config.json ...
I0820 01:50:39.736370    1611 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0820 01:50:39.736386    1611 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0820 01:50:39.736418    1611 cache.go:195] Successfully downloaded all kic artifacts
I0820 01:50:39.736493    1611 start.go:365] acquiring machines lock for minikube: {Name:mk55b05a674d8ff7df2072a41b8b6cd19f1028c3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0820 01:50:39.736662    1611 start.go:369] acquired machines lock for "minikube" in 152.8¬µs
I0820 01:50:39.736677    1611 start.go:96] Skipping create...Using existing machine configuration
I0820 01:50:39.736680    1611 fix.go:54] fixHost starting: 
I0820 01:50:39.736934    1611 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0820 01:50:39.995583    1611 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0820 01:50:39.995601    1611 fix.go:128] unexpected machine state, will restart: <nil>
I0820 01:50:40.034792    1611 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0820 01:50:40.134997    1611 cli_runner.go:164] Run: docker start minikube
I0820 01:50:42.215822    1611 cli_runner.go:217] Completed: docker start minikube: (2.080793077s)
I0820 01:50:42.215917    1611 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0820 01:50:42.445955    1611 kic.go:426] container "minikube" state is running.
I0820 01:50:42.446387    1611 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0820 01:50:42.684600    1611 profile.go:148] Saving config to /home/tapiwa/.minikube/profiles/minikube/config.json ...
I0820 01:50:42.684794    1611 machine.go:88] provisioning docker machine ...
I0820 01:50:42.684806    1611 ubuntu.go:169] provisioning hostname "minikube"
I0820 01:50:42.684858    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:42.933077    1611 main.go:141] libmachine: Using SSH client type: native
I0820 01:50:42.953272    1611 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0820 01:50:42.953287    1611 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0820 01:50:42.954390    1611 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:46560->127.0.0.1:32772: read: connection reset by peer
I0820 01:50:45.955807    1611 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:43626->127.0.0.1:32772: read: connection reset by peer
I0820 01:50:50.562815    1611 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0820 01:50:50.562894    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:50.774117    1611 main.go:141] libmachine: Using SSH client type: native
I0820 01:50:50.774538    1611 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0820 01:50:50.774551    1611 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0820 01:50:50.906944    1611 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0820 01:50:50.906967    1611 ubuntu.go:175] set auth options {CertDir:/home/tapiwa/.minikube CaCertPath:/home/tapiwa/.minikube/certs/ca.pem CaPrivateKeyPath:/home/tapiwa/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/tapiwa/.minikube/machines/server.pem ServerKeyPath:/home/tapiwa/.minikube/machines/server-key.pem ClientKeyPath:/home/tapiwa/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/tapiwa/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/tapiwa/.minikube}
I0820 01:50:50.906988    1611 ubuntu.go:177] setting up certificates
I0820 01:50:50.907001    1611 provision.go:83] configureAuth start
I0820 01:50:50.907055    1611 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0820 01:50:51.118863    1611 provision.go:138] copyHostCerts
I0820 01:50:51.119357    1611 exec_runner.go:144] found /home/tapiwa/.minikube/ca.pem, removing ...
I0820 01:50:51.119364    1611 exec_runner.go:203] rm: /home/tapiwa/.minikube/ca.pem
I0820 01:50:51.119459    1611 exec_runner.go:151] cp: /home/tapiwa/.minikube/certs/ca.pem --> /home/tapiwa/.minikube/ca.pem (1078 bytes)
I0820 01:50:51.135518    1611 exec_runner.go:144] found /home/tapiwa/.minikube/cert.pem, removing ...
I0820 01:50:51.135523    1611 exec_runner.go:203] rm: /home/tapiwa/.minikube/cert.pem
I0820 01:50:51.135565    1611 exec_runner.go:151] cp: /home/tapiwa/.minikube/certs/cert.pem --> /home/tapiwa/.minikube/cert.pem (1119 bytes)
I0820 01:50:51.136019    1611 exec_runner.go:144] found /home/tapiwa/.minikube/key.pem, removing ...
I0820 01:50:51.136024    1611 exec_runner.go:203] rm: /home/tapiwa/.minikube/key.pem
I0820 01:50:51.136053    1611 exec_runner.go:151] cp: /home/tapiwa/.minikube/certs/key.pem --> /home/tapiwa/.minikube/key.pem (1679 bytes)
I0820 01:50:51.136445    1611 provision.go:112] generating server cert: /home/tapiwa/.minikube/machines/server.pem ca-key=/home/tapiwa/.minikube/certs/ca.pem private-key=/home/tapiwa/.minikube/certs/ca-key.pem org=tapiwa.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0820 01:50:51.377124    1611 provision.go:172] copyRemoteCerts
I0820 01:50:51.377289    1611 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0820 01:50:51.377666    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:51.607636    1611 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/tapiwa/.minikube/machines/minikube/id_rsa Username:docker}
I0820 01:50:51.718365    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I0820 01:50:51.832035    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0820 01:50:51.879884    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0820 01:50:51.924521    1611 provision.go:86] duration metric: configureAuth took 1.017502904s
I0820 01:50:51.924540    1611 ubuntu.go:193] setting minikube options for container-runtime
I0820 01:50:51.924748    1611 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0820 01:50:51.924798    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:52.173863    1611 main.go:141] libmachine: Using SSH client type: native
I0820 01:50:52.174300    1611 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0820 01:50:52.174307    1611 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0820 01:50:52.413808    1611 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0820 01:50:52.413823    1611 ubuntu.go:71] root file system type: overlay
I0820 01:50:52.413932    1611 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0820 01:50:52.413981    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:52.643859    1611 main.go:141] libmachine: Using SSH client type: native
I0820 01:50:52.644288    1611 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0820 01:50:52.644350    1611 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0820 01:50:52.810840    1611 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0820 01:50:52.810940    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:53.055109    1611 main.go:141] libmachine: Using SSH client type: native
I0820 01:50:53.055574    1611 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80eb00] 0x811ba0 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0820 01:50:53.055588    1611 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0820 01:50:53.324937    1611 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0820 01:50:53.324957    1611 machine.go:91] provisioned docker machine in 10.640152696s
I0820 01:50:53.324966    1611 start.go:300] post-start starting for "minikube" (driver="docker")
I0820 01:50:53.324974    1611 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0820 01:50:53.325021    1611 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0820 01:50:53.325052    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:53.540873    1611 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/tapiwa/.minikube/machines/minikube/id_rsa Username:docker}
I0820 01:50:53.685349    1611 ssh_runner.go:195] Run: cat /etc/os-release
I0820 01:50:53.688947    1611 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0820 01:50:53.688969    1611 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0820 01:50:53.688976    1611 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0820 01:50:53.688981    1611 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0820 01:50:53.688991    1611 filesync.go:126] Scanning /home/tapiwa/.minikube/addons for local assets ...
I0820 01:50:53.697843    1611 filesync.go:126] Scanning /home/tapiwa/.minikube/files for local assets ...
I0820 01:50:53.698318    1611 start.go:303] post-start completed in 373.343658ms
I0820 01:50:53.698385    1611 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0820 01:50:53.698443    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:53.926694    1611 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/tapiwa/.minikube/machines/minikube/id_rsa Username:docker}
I0820 01:50:54.028586    1611 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0820 01:50:54.034734    1611 fix.go:56] fixHost completed within 14.298038362s
I0820 01:50:54.034754    1611 start.go:83] releasing machines lock for "minikube", held for 14.298081962s
I0820 01:50:54.034836    1611 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0820 01:50:54.260912    1611 ssh_runner.go:195] Run: cat /version.json
I0820 01:50:54.260955    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:54.261047    1611 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0820 01:50:54.261097    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:50:54.387727    1611 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/tapiwa/.minikube/machines/minikube/id_rsa Username:docker}
I0820 01:50:54.393198    1611 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/tapiwa/.minikube/machines/minikube/id_rsa Username:docker}
I0820 01:50:54.491766    1611 ssh_runner.go:195] Run: systemctl --version
I0820 01:50:58.840099    1611 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (4.57903249s)
I0820 01:50:58.840225    1611 ssh_runner.go:235] Completed: systemctl --version: (4.348442569s)
I0820 01:50:58.840308    1611 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0820 01:50:58.845071    1611 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0820 01:50:59.137444    1611 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0820 01:50:59.137512    1611 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0820 01:50:59.155407    1611 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0820 01:50:59.155427    1611 start.go:466] detecting cgroup driver to use...
I0820 01:50:59.155456    1611 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0820 01:50:59.155566    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0820 01:50:59.183828    1611 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0820 01:50:59.317111    1611 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0820 01:50:59.434844    1611 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0820 01:50:59.434918    1611 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0820 01:50:59.451144    1611 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0820 01:50:59.506584    1611 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0820 01:50:59.522418    1611 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0820 01:50:59.611206    1611 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0820 01:50:59.637716    1611 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0820 01:50:59.657001    1611 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0820 01:50:59.743348    1611 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0820 01:50:59.759690    1611 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0820 01:51:00.215925    1611 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0820 01:51:00.448251    1611 start.go:466] detecting cgroup driver to use...
I0820 01:51:00.448282    1611 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0820 01:51:00.448321    1611 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0820 01:51:00.467111    1611 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0820 01:51:00.467157    1611 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0820 01:51:00.574238    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0820 01:51:00.603418    1611 ssh_runner.go:195] Run: which cri-dockerd
I0820 01:51:00.607144    1611 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0820 01:51:00.622864    1611 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0820 01:51:00.653585    1611 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0820 01:51:00.909752    1611 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0820 01:51:01.048381    1611 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0820 01:51:01.048410    1611 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0820 01:51:01.076924    1611 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0820 01:51:01.186401    1611 ssh_runner.go:195] Run: sudo systemctl restart docker
I0820 01:51:07.921306    1611 ssh_runner.go:235] Completed: sudo systemctl restart docker: (6.73487801s)
I0820 01:51:07.921363    1611 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0820 01:51:08.034112    1611 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0820 01:51:08.179616    1611 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0820 01:51:08.304353    1611 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0820 01:51:08.422873    1611 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0820 01:51:08.448025    1611 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0820 01:51:08.556313    1611 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0820 01:51:10.585488    1611 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (2.029147186s)
I0820 01:51:10.585503    1611 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0820 01:51:10.585548    1611 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0820 01:51:10.589523    1611 start.go:534] Will wait 60s for crictl version
I0820 01:51:10.589561    1611 ssh_runner.go:195] Run: which crictl
I0820 01:51:10.593794    1611 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0820 01:51:12.406104    1611 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.812277059s)
I0820 01:51:12.406127    1611 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0820 01:51:12.406195    1611 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0820 01:51:13.201606    1611 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0820 01:51:13.320046    1611 out.go:204] üê≥  Preparing Kubernetes v1.27.3 on Docker 24.0.4 ...
I0820 01:51:13.320241    1611 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0820 01:51:13.693669    1611 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0820 01:51:13.697369    1611 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0820 01:51:13.721647    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0820 01:51:14.082604    1611 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0820 01:51:14.082702    1611 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0820 01:51:14.109331    1611 docker.go:636] Got preloaded images: -- stdout --
flatfourwrx/chatbot:latest
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0820 01:51:14.109344    1611 docker.go:566] Images already preloaded, skipping extraction
I0820 01:51:14.109389    1611 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0820 01:51:14.134093    1611 docker.go:636] Got preloaded images: -- stdout --
flatfourwrx/chatbot:latest
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0820 01:51:14.134111    1611 cache_images.go:84] Images are preloaded, skipping loading
I0820 01:51:14.134160    1611 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0820 01:51:15.928793    1611 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.794583195s)
I0820 01:51:15.928870    1611 cni.go:84] Creating CNI manager for ""
I0820 01:51:15.928890    1611 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0820 01:51:15.928913    1611 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0820 01:51:15.928945    1611 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0820 01:51:15.931497    1611 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0820 01:51:15.931738    1611 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0820 01:51:15.931856    1611 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.3
I0820 01:51:15.994504    1611 binaries.go:44] Found k8s binaries, skipping transfer
I0820 01:51:15.994595    1611 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0820 01:51:16.022005    1611 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0820 01:51:16.073307    1611 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0820 01:51:16.118053    1611 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0820 01:51:16.168299    1611 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0820 01:51:16.174153    1611 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0820 01:51:16.197948    1611 certs.go:56] Setting up /home/tapiwa/.minikube/profiles/minikube for IP: 192.168.49.2
I0820 01:51:16.197983    1611 certs.go:190] acquiring lock for shared ca certs: {Name:mkfaf418f53f6b1c0f025b43f7b2d6f70db43582 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0820 01:51:16.198258    1611 certs.go:199] skipping minikubeCA CA generation: /home/tapiwa/.minikube/ca.key
I0820 01:51:16.216714    1611 certs.go:199] skipping proxyClientCA CA generation: /home/tapiwa/.minikube/proxy-client-ca.key
I0820 01:51:16.216813    1611 certs.go:315] skipping minikube-user signed cert generation: /home/tapiwa/.minikube/profiles/minikube/client.key
I0820 01:51:16.217318    1611 certs.go:315] skipping minikube signed cert generation: /home/tapiwa/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0820 01:51:16.217678    1611 certs.go:315] skipping aggregator signed cert generation: /home/tapiwa/.minikube/profiles/minikube/proxy-client.key
I0820 01:51:16.217829    1611 certs.go:437] found cert: /home/tapiwa/.minikube/certs/home/tapiwa/.minikube/certs/ca-key.pem (1675 bytes)
I0820 01:51:16.217856    1611 certs.go:437] found cert: /home/tapiwa/.minikube/certs/home/tapiwa/.minikube/certs/ca.pem (1078 bytes)
I0820 01:51:16.217879    1611 certs.go:437] found cert: /home/tapiwa/.minikube/certs/home/tapiwa/.minikube/certs/cert.pem (1119 bytes)
I0820 01:51:16.217900    1611 certs.go:437] found cert: /home/tapiwa/.minikube/certs/home/tapiwa/.minikube/certs/key.pem (1679 bytes)
I0820 01:51:16.218780    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0820 01:51:16.291147    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0820 01:51:16.351255    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0820 01:51:16.399584    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0820 01:51:16.469678    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0820 01:51:16.534099    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0820 01:51:16.590259    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0820 01:51:16.652027    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0820 01:51:16.708875    1611 ssh_runner.go:362] scp /home/tapiwa/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0820 01:51:16.764802    1611 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0820 01:51:16.807279    1611 ssh_runner.go:195] Run: openssl version
I0820 01:51:16.908070    1611 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0820 01:51:16.942952    1611 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0820 01:51:16.948260    1611 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Aug 14 03:38 /usr/share/ca-certificates/minikubeCA.pem
I0820 01:51:16.948326    1611 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0820 01:51:16.956769    1611 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0820 01:51:16.981282    1611 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0820 01:51:16.999344    1611 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0820 01:51:17.010986    1611 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0820 01:51:17.021030    1611 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0820 01:51:17.032489    1611 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0820 01:51:17.042830    1611 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0820 01:51:17.052550    1611 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0820 01:51:17.064558    1611 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/tapiwa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0820 01:51:17.064711    1611 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0820 01:51:17.092564    1611 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0820 01:51:17.124831    1611 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0820 01:51:17.124849    1611 kubeadm.go:636] restartCluster start
I0820 01:51:17.124923    1611 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0820 01:51:17.154267    1611 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0820 01:51:17.154356    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0820 01:51:17.492080    1611 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:32769"
I0820 01:51:17.572187    1611 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0820 01:51:17.594514    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:17.594571    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:17.658439    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:18.159144    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:18.159230    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:18.184925    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:18.658690    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:18.658759    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:18.684931    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:19.159116    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:19.159232    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:19.183040    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:19.659674    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:19.659988    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:19.686973    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:20.159623    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:20.159693    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:20.181508    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:20.658862    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:20.659062    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:20.685364    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:21.158710    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:21.158788    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:21.184657    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:21.658686    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:21.658807    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:21.685549    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:22.158654    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:22.158719    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:22.183528    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:22.659270    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:22.659357    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:22.690526    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:23.159457    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:23.159522    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:23.183521    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:23.659656    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:23.659725    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:23.681095    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:24.158879    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:24.159243    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:24.186511    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:24.658935    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:24.659006    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:24.682679    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:25.159796    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:25.159992    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:25.183445    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:25.659393    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:25.659609    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:25.681441    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:26.159458    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:26.159612    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:26.186340    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:26.659136    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:26.659273    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:26.691696    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:27.159348    1611 api_server.go:166] Checking apiserver status ...
I0820 01:51:27.159467    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0820 01:51:27.185189    1611 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0820 01:51:27.594800    1611 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0820 01:51:27.594823    1611 kubeadm.go:1128] stopping kube-system containers ...
I0820 01:51:27.594945    1611 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0820 01:51:27.658331    1611 docker.go:462] Stopping containers: [75276d2616fb 20b0ce583c81 001406fc005c d19a932fb99c 619539a66bdd e59d758f9f32 472218a0b00d c33ed70697aa 3ba489f47f41 98dc20795736 471a77b54819 73d5528708d6 0a3a1f056a2e df8a6ee86ecc 7055162b04e2 2d659149cc91]
I0820 01:51:27.658395    1611 ssh_runner.go:195] Run: docker stop 75276d2616fb 20b0ce583c81 001406fc005c d19a932fb99c 619539a66bdd e59d758f9f32 472218a0b00d c33ed70697aa 3ba489f47f41 98dc20795736 471a77b54819 73d5528708d6 0a3a1f056a2e df8a6ee86ecc 7055162b04e2 2d659149cc91
I0820 01:51:27.724537    1611 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0820 01:51:27.754210    1611 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0820 01:51:27.790146    1611 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Aug 14 03:38 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Aug 14 03:38 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Aug 14 03:39 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Aug 14 03:38 /etc/kubernetes/scheduler.conf

I0820 01:51:27.790252    1611 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0820 01:51:27.862788    1611 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0820 01:51:27.922874    1611 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0820 01:51:28.025731    1611 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0820 01:51:28.025792    1611 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0820 01:51:28.044650    1611 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0820 01:51:28.063805    1611 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0820 01:51:28.063919    1611 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0820 01:51:28.085565    1611 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0820 01:51:28.106990    1611 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0820 01:51:28.107006    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0820 01:51:29.912578    1611 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (1.80554649s)
I0820 01:51:29.912626    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0820 01:51:31.196029    1611 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.283371908s)
I0820 01:51:31.196056    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0820 01:51:31.455634    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0820 01:51:31.546497    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0820 01:51:31.618831    1611 api_server.go:52] waiting for apiserver process to appear ...
I0820 01:51:31.618905    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:32.143972    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:32.643565    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:33.143518    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:33.644041    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:34.144503    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:34.644403    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:35.144067    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:35.644318    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:36.144046    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:36.643965    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:37.144303    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:37.644561    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:38.144367    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:38.643998    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:39.143690    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:39.644111    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:40.144187    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:40.643941    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:41.144135    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:41.644525    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:42.143535    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:42.644514    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:43.144025    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:43.644371    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:51:43.668187    1611 api_server.go:72] duration metric: took 12.049357215s to wait for apiserver process to appear ...
I0820 01:51:43.668204    1611 api_server.go:88] waiting for apiserver healthz status ...
I0820 01:51:43.668225    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:43.669885    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:43000->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:44.170737    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:44.171863    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35206->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:44.670820    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:44.671844    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35208->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:45.170391    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:45.171453    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35210->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:45.670786    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:45.671858    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35220->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:46.170395    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:46.171258    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35232->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:46.670398    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:46.673228    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35236->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:47.170342    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:47.171402    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35250->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:47.670053    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:47.671339    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35252->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:48.170800    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:48.171879    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35254->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:48.670561    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:48.671743    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35268->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:49.170490    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:49.172006    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35270->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:49.671101    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:49.672669    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35280->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:50.170090    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:50.320194    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35282->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:50.670998    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:50.672282    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35286->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:51.170393    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:51.174833    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35294->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:51.670629    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:51.671731    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35304->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:52.170216    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:52.172372    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35320->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:52.671210    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:52.672797    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35328->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:53.170684    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:53.172022    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35338->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:53.670557    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:53.672262    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:35348->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:54.170509    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:54.172216    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:41432->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:54.670518    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:54.672003    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:41448->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:55.170170    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:55.171785    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:41462->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:55.670737    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:55.672233    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:41468->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:56.170578    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:56.171994    1611 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:41470->127.0.0.1:32769: read: connection reset by peer
I0820 01:51:56.670838    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:51:59.978814    1611 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0820 01:51:59.978835    1611 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0820 01:51:59.978849    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:52:00.094804    1611 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0820 01:52:00.094846    1611 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0820 01:52:00.170792    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:52:00.185747    1611 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0820 01:52:00.185770    1611 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0820 01:52:00.670761    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:52:00.678188    1611 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0820 01:52:00.678202    1611 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0820 01:52:01.170112    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:52:01.186079    1611 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0820 01:52:01.186102    1611 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0820 01:52:01.670566    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:52:01.676969    1611 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0820 01:52:01.687977    1611 api_server.go:141] control plane version: v1.27.3
I0820 01:52:01.687998    1611 api_server.go:131] duration metric: took 18.019786713s to wait for apiserver health ...
I0820 01:52:01.688013    1611 cni.go:84] Creating CNI manager for ""
I0820 01:52:01.688032    1611 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0820 01:52:01.773367    1611 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0820 01:52:01.796299    1611 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0820 01:52:01.819170    1611 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0820 01:52:01.866701    1611 system_pods.go:43] waiting for kube-system pods to appear ...
I0820 01:52:01.917861    1611 system_pods.go:59] 7 kube-system pods found
I0820 01:52:01.917877    1611 system_pods.go:61] "coredns-5d78c9869d-9f9mw" [8390f277-defd-457b-8ee3-5302b7942610] Running
I0820 01:52:01.917881    1611 system_pods.go:61] "etcd-minikube" [684f48ee-a74f-47c1-aeaf-8bc01c6023de] Running
I0820 01:52:01.917884    1611 system_pods.go:61] "kube-apiserver-minikube" [b82b5e69-d236-42ed-b2bf-d5b8720cac5e] Running
I0820 01:52:01.917888    1611 system_pods.go:61] "kube-controller-manager-minikube" [658181ae-9361-42df-bae8-6ce6cad03649] Running
I0820 01:52:01.917891    1611 system_pods.go:61] "kube-proxy-972cs" [4841a6b1-0335-4d63-85e4-d15fd34868a7] Running
I0820 01:52:01.917896    1611 system_pods.go:61] "kube-scheduler-minikube" [eb589f7e-d41d-4be2-8114-5f1568ef2501] Running
I0820 01:52:01.917899    1611 system_pods.go:61] "storage-provisioner" [e9cebdee-b9ba-4a0d-8bc2-9466a1524b54] Running
I0820 01:52:01.917903    1611 system_pods.go:74] duration metric: took 51.185314ms to wait for pod list to return data ...
I0820 01:52:01.917908    1611 node_conditions.go:102] verifying NodePressure condition ...
I0820 01:52:01.937821    1611 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0820 01:52:01.938516    1611 node_conditions.go:123] node cpu capacity is 8
I0820 01:52:01.939867    1611 node_conditions.go:105] duration metric: took 21.928306ms to run NodePressure ...
I0820 01:52:01.939905    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0820 01:52:03.743152    1611 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.803225191s)
I0820 01:52:03.743172    1611 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0820 01:52:03.750777    1611 ops.go:34] apiserver oom_adj: -16
I0820 01:52:03.750793    1611 kubeadm.go:640] restartCluster took 46.625935482s
I0820 01:52:03.750804    1611 kubeadm.go:406] StartCluster complete in 46.686265112s
I0820 01:52:03.750833    1611 settings.go:142] acquiring lock: {Name:mk75fb956d8858482403ce55e0481290f8c9b356 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0820 01:52:03.751088    1611 settings.go:150] Updating kubeconfig:  /home/tapiwa/.kube/config
I0820 01:52:03.752039    1611 lock.go:35] WriteFile acquiring /home/tapiwa/.kube/config: {Name:mk63462ec948f5cc31d6d152be9a7dcdfa7107b4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0820 01:52:03.752448    1611 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0820 01:52:03.752680    1611 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0820 01:52:03.752726    1611 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0820 01:52:03.752858    1611 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0820 01:52:03.752873    1611 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0820 01:52:03.752878    1611 addons.go:240] addon storage-provisioner should already be in state true
I0820 01:52:03.752880    1611 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0820 01:52:03.752899    1611 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0820 01:52:03.752917    1611 host.go:66] Checking if "minikube" exists ...
I0820 01:52:03.753259    1611 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0820 01:52:03.753446    1611 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0820 01:52:03.780196    1611 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0820 01:52:03.780237    1611 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0820 01:52:03.943271    1611 out.go:177] üîé  Verifying Kubernetes components...
I0820 01:52:04.095950    1611 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0820 01:52:04.299389    1611 addons.go:240] addon default-storageclass should already be in state true
I0820 01:52:04.278484    1611 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0820 01:52:04.299565    1611 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0820 01:52:04.299658    1611 host.go:66] Checking if "minikube" exists ...
I0820 01:52:04.300190    1611 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0820 01:52:04.455847    1611 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0820 01:52:04.455860    1611 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0820 01:52:04.455939    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:52:04.846088    1611 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/tapiwa/.minikube/machines/minikube/id_rsa Username:docker}
I0820 01:52:04.854321    1611 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0820 01:52:04.866160    1611 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0820 01:52:04.866331    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0820 01:52:05.140541    1611 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0820 01:52:05.343725    1611 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/tapiwa/.minikube/machines/minikube/id_rsa Username:docker}
I0820 01:52:05.546076    1611 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0820 01:52:08.301721    1611 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (4.549247938s)
I0820 01:52:08.301802    1611 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0820 01:52:08.301876    1611 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (4.002415289s)
I0820 01:52:08.301944    1611 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0820 01:52:08.689682    1611 api_server.go:52] waiting for apiserver process to appear ...
I0820 01:52:08.689784    1611 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0820 01:52:11.167692    1611 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.02712157s)
I0820 01:52:11.167796    1611 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (5.621698554s)
I0820 01:52:11.414133    1611 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0820 01:52:11.167915    1611 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.478109082s)
I0820 01:52:11.414224    1611 api_server.go:72] duration metric: took 7.633942477s to wait for apiserver process to appear ...
I0820 01:52:11.734950    1611 api_server.go:88] waiting for apiserver healthz status ...
I0820 01:52:11.734981    1611 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0820 01:52:11.734987    1611 addons.go:502] enable addons completed in 7.982371754s: enabled=[storage-provisioner default-storageclass]
I0820 01:52:12.004406    1611 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0820 01:52:12.006362    1611 api_server.go:141] control plane version: v1.27.3
I0820 01:52:12.006381    1611 api_server.go:131] duration metric: took 271.418774ms to wait for apiserver health ...
I0820 01:52:12.006387    1611 system_pods.go:43] waiting for kube-system pods to appear ...
I0820 01:52:12.018287    1611 system_pods.go:59] 7 kube-system pods found
I0820 01:52:12.018303    1611 system_pods.go:61] "coredns-5d78c9869d-9f9mw" [8390f277-defd-457b-8ee3-5302b7942610] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0820 01:52:12.018308    1611 system_pods.go:61] "etcd-minikube" [684f48ee-a74f-47c1-aeaf-8bc01c6023de] Running
I0820 01:52:12.018311    1611 system_pods.go:61] "kube-apiserver-minikube" [b82b5e69-d236-42ed-b2bf-d5b8720cac5e] Running
I0820 01:52:12.018315    1611 system_pods.go:61] "kube-controller-manager-minikube" [658181ae-9361-42df-bae8-6ce6cad03649] Running
I0820 01:52:12.018319    1611 system_pods.go:61] "kube-proxy-972cs" [4841a6b1-0335-4d63-85e4-d15fd34868a7] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0820 01:52:12.018323    1611 system_pods.go:61] "kube-scheduler-minikube" [eb589f7e-d41d-4be2-8114-5f1568ef2501] Running
I0820 01:52:12.018327    1611 system_pods.go:61] "storage-provisioner" [e9cebdee-b9ba-4a0d-8bc2-9466a1524b54] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0820 01:52:12.018334    1611 system_pods.go:74] duration metric: took 11.941698ms to wait for pod list to return data ...
I0820 01:52:12.018342    1611 kubeadm.go:581] duration metric: took 8.238067718s to wait for : map[apiserver:true system_pods:true] ...
I0820 01:52:12.018353    1611 node_conditions.go:102] verifying NodePressure condition ...
I0820 01:52:12.023319    1611 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0820 01:52:12.023335    1611 node_conditions.go:123] node cpu capacity is 8
I0820 01:52:12.023344    1611 node_conditions.go:105] duration metric: took 4.9881ms to run NodePressure ...
I0820 01:52:12.023355    1611 start.go:228] waiting for startup goroutines ...
I0820 01:52:12.023360    1611 start.go:233] waiting for cluster config update ...
I0820 01:52:12.023369    1611 start.go:242] writing updated cluster config ...
I0820 01:52:12.023722    1611 ssh_runner.go:195] Run: rm -f paused
I0820 01:52:12.091116    1611 start.go:596] kubectl: 1.27.4, cluster: 1.27.3 (minor skew: 0)
I0820 01:52:12.447263    1611 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Aug 20 06:51:05 minikube dockerd[369]: time="2023-08-20T06:51:05.199485320Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Aug 20 06:51:05 minikube dockerd[369]: time="2023-08-20T06:51:05.200352920Z" level=info msg="Daemon shutdown complete"
Aug 20 06:51:05 minikube systemd[1]: docker.service: Deactivated successfully.
Aug 20 06:51:05 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 20 06:51:05 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 20 06:51:05 minikube dockerd[797]: time="2023-08-20T06:51:05.274503143Z" level=info msg="Starting up"
Aug 20 06:51:05 minikube dockerd[797]: time="2023-08-20T06:51:05.341958164Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 20 06:51:05 minikube dockerd[797]: time="2023-08-20T06:51:05.649605761Z" level=info msg="Loading containers: start."
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.430883522Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.608114578Z" level=info msg="Loading containers: done."
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.718869013Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.718898113Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.718904213Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.718908913Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.718926813Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.719005613Z" level=info msg="Daemon has completed initialization"
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.916129975Z" level=info msg="API listen on /var/run/docker.sock"
Aug 20 06:51:07 minikube dockerd[797]: time="2023-08-20T06:51:07.916262875Z" level=info msg="API listen on [::]:2376"
Aug 20 06:51:07 minikube systemd[1]: Started Docker Application Container Engine.
Aug 20 06:51:08 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Start docker client with request timeout 0s"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Loaded network plugin cni"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Docker Info: &{ID:b2e42d40-c507-4b63-82c6-0fd25eb542e4 Containers:22 ContainersRunning:0 ContainersPaused:0 ContainersStopped:22 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:36 SystemTime:2023-08-20T06:51:10.508679681Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.2 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0002e22a0 NCPU:8 MemTotal:8269004800 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 20 06:51:10 minikube cri-dockerd[1073]: time="2023-08-20T06:51:10Z" level=info msg="Start cri-dockerd grpc backend"
Aug 20 06:51:10 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 20 06:51:36 minikube cri-dockerd[1073]: time="2023-08-20T06:51:36Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"chatbot-68b6449bc6-4bvp4_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b1c6146d79b058ec531da9324ce2119e06d807e8d01f104a4991dbb71c2449ec\""
Aug 20 06:51:36 minikube cri-dockerd[1073]: time="2023-08-20T06:51:36Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"chatbot-68b6449bc6-87ndt_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b38043265fdc85ccd0658cc96a1f56a857646ba09717ce80c905a38205052a2c\""
Aug 20 06:51:36 minikube cri-dockerd[1073]: time="2023-08-20T06:51:36Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-9f9mw_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"619539a66bddf943a10366af002a0b478d457fb1a215e39c171c3df73ffaeac3\""
Aug 20 06:51:36 minikube cri-dockerd[1073]: time="2023-08-20T06:51:36Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"chatbot-68b6449bc6-4dn2p_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"303b0105ee56d51c755957bafb124e9e73c677a2bcbd3e43e8efdbffe96a2f9f\""
Aug 20 06:51:40 minikube cri-dockerd[1073]: time="2023-08-20T06:51:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0b8033c5f7d6056ded47fd08b1396e23c4eb1d03d25edf92088b4ad1eb76e8c8/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Aug 20 06:51:40 minikube cri-dockerd[1073]: time="2023-08-20T06:51:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6ab5bd37aaffe989056eea40e9362b6e12ac185595c1782b5378fff2becbc360/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Aug 20 06:51:40 minikube cri-dockerd[1073]: time="2023-08-20T06:51:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/356063f26f4468b07f2c4d34e8405d6e48f8ecd968f63cdbea8f1cd39ba191ce/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Aug 20 06:51:40 minikube cri-dockerd[1073]: time="2023-08-20T06:51:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1e6e2fafb3a73ce2cf8a63964f18562ca800ed367b7c716d8a62ef0aa35f19e/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Aug 20 06:52:02 minikube cri-dockerd[1073]: time="2023-08-20T06:52:02Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 20 06:52:15 minikube cri-dockerd[1073]: time="2023-08-20T06:52:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dfdfedf6382019ad095055c1d565adc175010388b47a04f9ba2ca49033e32e34/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Aug 20 06:52:15 minikube cri-dockerd[1073]: time="2023-08-20T06:52:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b5a934641afe6fc6030157ea80171ab01af72df5caae84c028b7fb849cce0473/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Aug 20 06:52:18 minikube cri-dockerd[1073]: time="2023-08-20T06:52:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/652c1172c498cb308085c9858c36466bf987e2c975104880fa9b7a10ec008ed1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 20 06:52:18 minikube cri-dockerd[1073]: time="2023-08-20T06:52:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47369990d3fc65185c3fccb3d640b46f01d2d74278c5cde2726d9c654ecf66c6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 20 06:52:18 minikube cri-dockerd[1073]: time="2023-08-20T06:52:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e980a6475fe9af671e56131385e433f57136234d3e180d333bc9e1656af36eb3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 20 06:52:18 minikube cri-dockerd[1073]: time="2023-08-20T06:52:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e1555ca91706ce6c62e3ffc691e27942e8d98ef47d6daf3e16d9a448ba1ad9e0/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Aug 20 06:52:24 minikube cri-dockerd[1073]: time="2023-08-20T06:52:24Z" level=info msg="Stop pulling image flatfourwrx/chatbot:latest: Status: Image is up to date for flatfourwrx/chatbot:latest"
Aug 20 06:52:26 minikube cri-dockerd[1073]: time="2023-08-20T06:52:26Z" level=info msg="Stop pulling image flatfourwrx/chatbot:latest: Status: Image is up to date for flatfourwrx/chatbot:latest"
Aug 20 06:52:29 minikube cri-dockerd[1073]: time="2023-08-20T06:52:29Z" level=info msg="Stop pulling image flatfourwrx/chatbot:latest: Status: Image is up to date for flatfourwrx/chatbot:latest"
Aug 20 06:52:56 minikube dockerd[797]: time="2023-08-20T06:52:56.260408743Z" level=info msg="ignoring event" container=b5ef6d023aa0488f81c0e4459289079d1a40f71a4d1d432aba1f67fc0aae1cfa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 20 06:53:01 minikube dockerd[797]: time="2023-08-20T06:53:01.836074226Z" level=info msg="ignoring event" container=fd189d07b74631f694ef59ef521dc1ad07053598a414d3d8b7acfcdf5b14adb9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 20 06:53:01 minikube dockerd[797]: time="2023-08-20T06:53:01.836198226Z" level=info msg="ignoring event" container=cf24d569107485350fd6c1b6c0f729bd84b5c7a701cfbee38ea14ab09af84e88 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 20 06:53:01 minikube dockerd[797]: time="2023-08-20T06:53:01.837702226Z" level=info msg="ignoring event" container=6c5c3a42c3a0b24e65bbb84ae7da2c2d534fd5565777dc7fc8024dd3ef5d42d2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 20 06:53:04 minikube dockerd[797]: time="2023-08-20T06:53:04.233702537Z" level=error msg="Failed to compute size of container rootfs 286de9213b03a92a63c4b3cc02472e1e8570f4f1a99804f188da0b5201a39895: mount does not exist"
Aug 20 06:53:19 minikube cri-dockerd[1073]: time="2023-08-20T06:53:19Z" level=info msg="Stop pulling image flatfourwrx/chatbot:latest: Status: Image is up to date for flatfourwrx/chatbot:latest"
Aug 20 06:53:20 minikube cri-dockerd[1073]: time="2023-08-20T06:53:20Z" level=info msg="Stop pulling image flatfourwrx/chatbot:latest: Status: Image is up to date for flatfourwrx/chatbot:latest"
Aug 20 06:53:22 minikube cri-dockerd[1073]: time="2023-08-20T06:53:22Z" level=info msg="Stop pulling image flatfourwrx/chatbot:latest: Status: Image is up to date for flatfourwrx/chatbot:latest"
Aug 20 07:11:20 minikube cri-dockerd[1073]: time="2023-08-20T07:11:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/290e0f7639eb7332e66f7d2c371f69c728e4d8f0e493a6a478f2d13c49e04e9a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 20 07:11:33 minikube cri-dockerd[1073]: time="2023-08-20T07:11:33Z" level=info msg="Pulling image nginx:latest: 52d2b7f179e3: Extracting [================================================>  ]  28.02MB/29.12MB"
Aug 20 07:11:42 minikube cri-dockerd[1073]: time="2023-08-20T07:11:42Z" level=info msg="Stop pulling image nginx:latest: Status: Downloaded newer image for nginx:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                         CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
478b0d932361a       nginx@sha256:104c7c5c54f2685f0f46f3be607ce60da7085da3eaa5ad22d3d9f01594295e9c                 36 minutes ago      Running             nginx                     0                   290e0f7639eb7       nginx-depl-6b7698588c-n46d4
171bf6550acdb       6e38f40d628db                                                                                 55 minutes ago      Running             storage-provisioner       5                   b5a934641afe6       storage-provisioner
bd30320462fe3       flatfourwrx/chatbot@sha256:8861d767333b9890f7d98496e6ca8246b333dd6f3bfbfda8b64ea98fa3c7a010   55 minutes ago      Running             chatbot                   2                   652c1172c498c       chatbot-68b6449bc6-4dn2p
1cae829e0894f       flatfourwrx/chatbot@sha256:8861d767333b9890f7d98496e6ca8246b333dd6f3bfbfda8b64ea98fa3c7a010   55 minutes ago      Running             chatbot                   2                   47369990d3fc6       chatbot-68b6449bc6-87ndt
24720a74e6b5b       flatfourwrx/chatbot@sha256:8861d767333b9890f7d98496e6ca8246b333dd6f3bfbfda8b64ea98fa3c7a010   55 minutes ago      Running             chatbot                   2                   e980a6475fe9a       chatbot-68b6449bc6-4bvp4
6c5c3a42c3a0b       flatfourwrx/chatbot@sha256:8861d767333b9890f7d98496e6ca8246b333dd6f3bfbfda8b64ea98fa3c7a010   56 minutes ago      Exited              chatbot                   1                   652c1172c498c       chatbot-68b6449bc6-4dn2p
f6bc377ad2e6b       5780543258cf0                                                                                 56 minutes ago      Running             kube-proxy                1                   dfdfedf638201       kube-proxy-972cs
cf24d56910748       flatfourwrx/chatbot@sha256:8861d767333b9890f7d98496e6ca8246b333dd6f3bfbfda8b64ea98fa3c7a010   56 minutes ago      Exited              chatbot                   1                   e980a6475fe9a       chatbot-68b6449bc6-4bvp4
fd189d07b7463       flatfourwrx/chatbot@sha256:8861d767333b9890f7d98496e6ca8246b333dd6f3bfbfda8b64ea98fa3c7a010   56 minutes ago      Exited              chatbot                   1                   47369990d3fc6       chatbot-68b6449bc6-87ndt
b5ef6d023aa04       6e38f40d628db                                                                                 56 minutes ago      Exited              storage-provisioner       4                   b5a934641afe6       storage-provisioner
b9eca4981a3db       ead0a4a53df89                                                                                 56 minutes ago      Running             coredns                   1                   e1555ca91706c       coredns-5d78c9869d-9f9mw
62ce3e4f4b44f       08a0c939e61b7                                                                                 56 minutes ago      Running             kube-apiserver            1                   a1e6e2fafb3a7       kube-apiserver-minikube
c384b393219ce       86b6af7dd652c                                                                                 56 minutes ago      Running             etcd                      1                   356063f26f446       etcd-minikube
3a326daed7030       41697ceeb70b3                                                                                 56 minutes ago      Running             kube-scheduler            1                   0b8033c5f7d60       kube-scheduler-minikube
4289a42d67a1d       7cffc01dba0e1                                                                                 56 minutes ago      Running             kube-controller-manager   2                   6ab5bd37aaffe       kube-controller-manager-minikube
001406fc005cb       ead0a4a53df89                                                                                 6 days ago          Exited              coredns                   0                   619539a66bddf       coredns-5d78c9869d-9f9mw
d19a932fb99cb       5780543258cf0                                                                                 6 days ago          Exited              kube-proxy                0                   472218a0b00d8       kube-proxy-972cs
c33ed70697aa3       7cffc01dba0e1                                                                                 6 days ago          Exited              kube-controller-manager   1                   0a3a1f056a2e0       kube-controller-manager-minikube
3ba489f47f415       41697ceeb70b3                                                                                 6 days ago          Exited              kube-scheduler            0                   2d659149cc914       kube-scheduler-minikube
98dc207957366       86b6af7dd652c                                                                                 6 days ago          Exited              etcd                      0                   7055162b04e26       etcd-minikube
73d5528708d66       08a0c939e61b7                                                                                 6 days ago          Exited              kube-apiserver            0                   df8a6ee86ecc6       kube-apiserver-minikube

* 
* ==> coredns [001406fc005c] <==
* [INFO] 10.244.0.10:52315 - 15212 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000509s
[INFO] 10.244.0.10:44135 - 45638 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.000127s
[INFO] 10.244.0.10:44135 - 3144 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0001702s
[INFO] 10.244.0.11:35012 - 23169 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001914s
[INFO] 10.244.0.11:35012 - 31106 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0002361s
[INFO] 10.244.0.11:40166 - 41308 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0000863s
[INFO] 10.244.0.11:40166 - 38945 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000156s
[INFO] 10.244.0.11:35212 - 38832 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000996s
[INFO] 10.244.0.11:35212 - 22205 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0002224s
[INFO] 10.244.0.11:59280 - 36522 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.0001576s
[INFO] 10.244.0.11:59280 - 39080 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0002667s
[INFO] 10.244.0.10:35148 - 7184 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001696s
[INFO] 10.244.0.10:35148 - 56605 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001673s
[INFO] 10.244.0.10:52159 - 28536 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002688s
[INFO] 10.244.0.10:52159 - 56188 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0004334s
[INFO] 10.244.0.10:58855 - 27897 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000872s
[INFO] 10.244.0.10:58855 - 9210 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001111s
[INFO] 10.244.0.10:38436 - 51683 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.0001222s
[INFO] 10.244.0.10:38436 - 36456 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0002075s
[INFO] 10.244.0.11:50882 - 30878 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0004181s
[INFO] 10.244.0.11:50882 - 664 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0002664s
[INFO] 10.244.0.11:47246 - 31063 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001777s
[INFO] 10.244.0.11:47246 - 59478 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002568s
[INFO] 10.244.0.11:40332 - 20743 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001241s
[INFO] 10.244.0.11:40332 - 47877 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000178s
[INFO] 10.244.0.11:40847 - 8405 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.000124s
[INFO] 10.244.0.11:40847 - 58824 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0002109s
[INFO] 10.244.0.9:34856 - 11540 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001841s
[INFO] 10.244.0.9:34856 - 47129 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0003158s
[INFO] 10.244.0.9:60410 - 12178 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001364s
[INFO] 10.244.0.9:60410 - 60565 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002456s
[INFO] 10.244.0.9:53073 - 59346 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000456s
[INFO] 10.244.0.9:53073 - 9684 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000859s
[INFO] 10.244.0.9:35066 - 62507 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.0001339s
[INFO] 10.244.0.9:35066 - 59945 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.000212001s
[INFO] 10.244.0.11:38268 - 36183 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0002371s
[INFO] 10.244.0.11:38268 - 26196 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000432s
[INFO] 10.244.0.11:53345 - 41848 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002838s
[INFO] 10.244.0.11:53345 - 9060 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0004482s
[INFO] 10.244.0.11:49387 - 29759 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0013705s
[INFO] 10.244.0.11:49387 - 52992 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.006399201s
[INFO] 10.244.0.11:49659 - 3242 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 207 0.003381101s
[INFO] 10.244.0.11:49659 - 49109 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 255 0.004013501s
[INFO] 10.244.0.10:58620 - 54115 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001816s
[INFO] 10.244.0.10:58620 - 39783 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000186s
[INFO] 10.244.0.10:51230 - 32804 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0000961s
[INFO] 10.244.0.10:51230 - 32 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001704s
[INFO] 10.244.0.10:57612 - 7330 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0002374s
[INFO] 10.244.0.10:57612 - 38062 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000343s
[INFO] 10.244.0.10:46675 - 1585 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0001317s
[INFO] 10.244.0.10:46675 - 50486 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.0002818s
[INFO] 10.244.0.9:49058 - 12438 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0002406s
[INFO] 10.244.0.9:49058 - 1692 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.000387s
[INFO] 10.244.0.9:35503 - 15990 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0000895s
[INFO] 10.244.0.9:35503 - 41333 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002594s
[INFO] 10.244.0.9:34572 - 53231 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000781s
[INFO] 10.244.0.9:34572 - 23314 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001848s
[INFO] 10.244.0.9:56990 - 61596 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.000104s
[INFO] 10.244.0.9:56990 - 48019 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0001267s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.200435444s

* 
* ==> coredns [b9eca4981a3d] <==
* [INFO] 10.244.0.12:44317 - 39458 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000151s
[INFO] 10.244.0.12:44317 - 65086 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000457s
[INFO] 10.244.0.12:53674 - 13356 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.0001098s
[INFO] 10.244.0.12:53674 - 5935 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0004111s
[INFO] 10.244.0.13:34025 - 36946 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001482s
[INFO] 10.244.0.13:34025 - 46933 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001966s
[INFO] 10.244.0.13:52322 - 5023 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001136s
[INFO] 10.244.0.13:52322 - 65178 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002055s
[INFO] 10.244.0.13:39099 - 48939 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000881s
[INFO] 10.244.0.13:39099 - 17942 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001062s
[INFO] 10.244.0.13:45625 - 1200 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.0001059s
[INFO] 10.244.0.13:45625 - 25262 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0001196s
[INFO] 10.244.0.12:34813 - 52908 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0000927s
[INFO] 10.244.0.12:34813 - 5294 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0001336s
[INFO] 10.244.0.12:50747 - 43539 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001174s
[INFO] 10.244.0.12:50747 - 23565 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002194s
[INFO] 10.244.0.12:39010 - 16354 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001002s
[INFO] 10.244.0.12:39010 - 18912 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001712s
[INFO] 10.244.0.12:56594 - 51443 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.000123s
[INFO] 10.244.0.12:56594 - 526 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0002292s
[INFO] 10.244.0.12:60698 - 52865 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0002246s
[INFO] 10.244.0.12:60698 - 20356 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0003147s
[INFO] 10.244.0.12:41320 - 57423 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001348s
[INFO] 10.244.0.12:41320 - 16196 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002379s
[INFO] 10.244.0.12:45998 - 23771 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0000822s
[INFO] 10.244.0.12:45998 - 11941 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001462s
[INFO] 10.244.0.12:60295 - 65297 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 207 0.0019828s
[INFO] 10.244.0.12:60295 - 57370 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 255 0.0021576s
[INFO] 10.244.0.12:51638 - 6078 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0004136s
[INFO] 10.244.0.12:51638 - 38069 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.0005662s
[INFO] 10.244.0.12:40935 - 4138 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0002116s
[INFO] 10.244.0.12:40935 - 29481 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0003757s
[INFO] 10.244.0.12:52119 - 46761 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001853s
[INFO] 10.244.0.12:52119 - 685 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0004065s
[INFO] 10.244.0.12:46363 - 39821 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 255 0.000116s
[INFO] 10.244.0.12:46363 - 61939 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,aa,rd 207 0.0003254s
[INFO] 10.244.0.14:57868 - 20754 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.007119201s
[INFO] 10.244.0.12:55011 - 40078 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.004823801s
[INFO] 10.244.0.14:57868 - 61935 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.007170901s
[INFO] 10.244.0.13:38953 - 28830 "AAAA IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.005791901s
[INFO] 10.244.0.13:54484 - 32583 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001543s
[INFO] 10.244.0.13:38953 - 64931 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.005777401s
[INFO] 10.244.0.13:54484 - 27467 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.000464s
[INFO] 10.244.0.13:45748 - 30508 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001125s
[INFO] 10.244.0.12:55011 - 47856 "A IN raw.githubusercontent.com.default.svc.cluster.local. udp 69 false 512" NXDOMAIN qr,aa,rd 162 0.006683401s
[INFO] 10.244.0.13:45748 - 48163 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0002806s
[INFO] 10.244.0.14:49012 - 6885 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0004201s
[INFO] 10.244.0.14:49012 - 48363 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0004677s
[INFO] 10.244.0.12:54212 - 55609 "AAAA IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0000711s
[INFO] 10.244.0.12:54212 - 43317 "A IN raw.githubusercontent.com.svc.cluster.local. udp 61 false 512" NXDOMAIN qr,aa,rd 154 0.0001084s
[INFO] 10.244.0.12:55074 - 42672 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.000088s
[INFO] 10.244.0.14:45390 - 13768 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0001002s
[INFO] 10.244.0.12:55074 - 5053 "A IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0002248s
[INFO] 10.244.0.14:45390 - 31941 "AAAA IN raw.githubusercontent.com.cluster.local. udp 57 false 512" NXDOMAIN qr,aa,rd 150 0.0003907s
[INFO] 10.244.0.14:60993 - 8504 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 255 0.020720803s
[INFO] 10.244.0.12:52568 - 60977 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 255 0.021067103s
[INFO] 10.244.0.13:34516 - 40417 "AAAA IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 255 0.022237803s
[INFO] 10.244.0.12:52568 - 3278 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 207 0.021762903s
[INFO] 10.244.0.13:34516 - 51171 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 207 0.022880403s
[INFO] 10.244.0.14:60993 - 14138 "A IN raw.githubusercontent.com. udp 43 false 512" NOERROR qr,rd 207 0.021409603s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd3f3801765d093a485d255043149f92ec0a695f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_13T22_39_20_0700
                    minikube.k8s.io/version=v1.31.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 14 Aug 2023 03:38:49 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 20 Aug 2023 07:48:36 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 20 Aug 2023 07:47:56 +0000   Mon, 14 Aug 2023 03:38:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 20 Aug 2023 07:47:56 +0000   Mon, 14 Aug 2023 03:38:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 20 Aug 2023 07:47:56 +0000   Mon, 14 Aug 2023 03:38:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 20 Aug 2023 07:47:56 +0000   Mon, 14 Aug 2023 03:38:52 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8075200Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8075200Ki
  pods:               110
System Info:
  Machine ID:                 bcaed82557ac4a5987fc01f0d6261ec7
  System UUID:                bcaed82557ac4a5987fc01f0d6261ec7
  Boot ID:                    37e3f7d1-3bac-4d22-af0a-900784ec3275
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.3
  Kube-Proxy Version:         v1.27.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     chatbot-68b6449bc6-4bvp4            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d2h
  default                     chatbot-68b6449bc6-4dn2p            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d2h
  default                     chatbot-68b6449bc6-87ndt            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d2h
  default                     nginx-depl-6b7698588c-n46d4         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         37m
  kube-system                 coredns-5d78c9869d-9f9mw            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     6d4h
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         6d4h
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d4h
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d4h
  kube-system                 kube-proxy-972cs                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d4h
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d4h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d4h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 56m                kube-proxy       
  Normal  Starting                 57m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  57m (x8 over 57m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    57m (x8 over 57m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     57m (x7 over 57m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  57m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           56m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Aug20 06:01] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000]  #2 #3 #4 #5 #6 #7
[  +0.020780] PCI: Fatal: No config space access function found
[  +0.029339] PCI: System does not support PCI
[  +0.047462] kvm: no hardware support
[  +0.000007] kvm: no hardware support
[Aug20 06:02] FS-Cache: Duplicate cookie detected
[  +0.002109] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000466] FS-Cache: O-cookie d=000000003cb7951c{9P.session} n=000000001b2855e0
[  +0.000637] FS-Cache: O-key=[10] '34323934393337373039'
[  +0.000400] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000488] FS-Cache: N-cookie d=000000003cb7951c{9P.session} n=000000007549f747
[  +0.000556] FS-Cache: N-key=[10] '34323934393337373039'
[  +8.060992] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000654] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000570] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000518] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000785] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.255301] Failed to connect to bus: No such file or directory
[  +0.254285] Failed to connect to bus: No such file or directory
[  +0.253658] Failed to connect to bus: No such file or directory
[  +0.253412] Failed to connect to bus: No such file or directory
[  +0.253794] Failed to connect to bus: No such file or directory
[  +0.253501] Failed to connect to bus: No such file or directory
[  +0.253519] Failed to connect to bus: No such file or directory
[  +0.254179] Failed to connect to bus: No such file or directory
[  +0.254249] Failed to connect to bus: No such file or directory
[  +0.253503] Failed to connect to bus: No such file or directory
[  +0.254682] Failed to connect to bus: No such file or directory
[  +0.614790] systemd-journald[91]: File /var/log/journal/5acafb90c1c4470ab8ce90a17deef990/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +4.334355] WSL (2) ERROR: WaitForBootProcess:3184: /sbin/init failed to start within 10000
[  +0.000006] ms
[  +9.158222] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.

* 
* ==> etcd [98dc20795736] <==
* {"level":"warn","ts":"2023-08-18T01:56:36.062Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T01:56:35.637Z","time spent":"425.027119ms","remote":"127.0.0.1:53604","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:143292 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-08-18T01:56:36.062Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"234.64821ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-08-18T01:56:36.062Z","caller":"traceutil/trace.go:171","msg":"trace[1910734486] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:143293; }","duration":"234.69301ms","start":"2023-08-18T01:56:35.827Z","end":"2023-08-18T01:56:36.062Z","steps":["trace[1910734486] 'agreement among raft nodes before linearized reading'  (duration: 234.57311ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T01:56:36.062Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"290.236513ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-08-18T01:56:36.062Z","caller":"traceutil/trace.go:171","msg":"trace[1883589406] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:143293; }","duration":"290.347213ms","start":"2023-08-18T01:56:35.772Z","end":"2023-08-18T01:56:36.062Z","steps":["trace[1883589406] 'agreement among raft nodes before linearized reading'  (duration: 290.161813ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T01:56:38.176Z","caller":"traceutil/trace.go:171","msg":"trace[1003817929] transaction","detail":"{read_only:false; response_revision:143294; number_of_response:1; }","duration":"107.433205ms","start":"2023-08-18T01:56:38.068Z","end":"2023-08-18T01:56:38.176Z","steps":["trace[1003817929] 'process raft request'  (duration: 107.295905ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T01:56:49.802Z","caller":"traceutil/trace.go:171","msg":"trace[1080343969] transaction","detail":"{read_only:false; response_revision:143305; number_of_response:1; }","duration":"126.573202ms","start":"2023-08-18T01:56:49.675Z","end":"2023-08-18T01:56:49.802Z","steps":["trace[1080343969] 'process raft request'  (duration: 126.392502ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T01:57:15.400Z","caller":"traceutil/trace.go:171","msg":"trace[337349648] transaction","detail":"{read_only:false; response_revision:143324; number_of_response:1; }","duration":"144.900489ms","start":"2023-08-18T01:57:15.255Z","end":"2023-08-18T01:57:15.400Z","steps":["trace[337349648] 'process raft request'  (duration: 144.629489ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T01:57:40.889Z","caller":"traceutil/trace.go:171","msg":"trace[300693165] transaction","detail":"{read_only:false; response_revision:143345; number_of_response:1; }","duration":"105.996904ms","start":"2023-08-18T01:57:40.783Z","end":"2023-08-18T01:57:40.889Z","steps":["trace[300693165] 'process raft request'  (duration: 105.676804ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T01:58:17.416Z","caller":"traceutil/trace.go:171","msg":"trace[2048936554] transaction","detail":"{read_only:false; response_revision:143372; number_of_response:1; }","duration":"303.569243ms","start":"2023-08-18T01:58:17.113Z","end":"2023-08-18T01:58:17.416Z","steps":["trace[2048936554] 'process raft request'  (duration: 303.470543ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T01:58:17.416Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T01:58:17.113Z","time spent":"303.659143ms","remote":"127.0.0.1:53604","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:143371 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-08-18T01:59:08.118Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"147.547295ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128023100508553432 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:143182 > ","response":"size:6"}
{"level":"info","ts":"2023-08-18T01:59:08.118Z","caller":"traceutil/trace.go:171","msg":"trace[1181060136] compact","detail":"{revision:143182; response_revision:143412; }","duration":"176.463094ms","start":"2023-08-18T01:59:07.941Z","end":"2023-08-18T01:59:08.118Z","steps":["trace[1181060136] 'process raft request'  (duration: 28.793899ms)","trace[1181060136] 'check and update compact revision'  (duration: 147.438095ms)"],"step_count":2}
{"level":"info","ts":"2023-08-18T01:59:08.118Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":143182}
{"level":"info","ts":"2023-08-18T01:59:08.119Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":143182,"took":"745.7¬µs","hash":3209383048}
{"level":"info","ts":"2023-08-18T01:59:08.119Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3209383048,"revision":143182,"compact-revision":142951}
{"level":"info","ts":"2023-08-18T01:59:52.440Z","caller":"traceutil/trace.go:171","msg":"trace[2095754420] transaction","detail":"{read_only:false; response_revision:143447; number_of_response:1; }","duration":"101.6929ms","start":"2023-08-18T01:59:52.338Z","end":"2023-08-18T01:59:52.440Z","steps":["trace[2095754420] 'process raft request'  (duration: 101.3521ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T01:59:56.632Z","caller":"traceutil/trace.go:171","msg":"trace[1314604589] transaction","detail":"{read_only:false; response_revision:143451; number_of_response:1; }","duration":"131.006303ms","start":"2023-08-18T01:59:56.501Z","end":"2023-08-18T01:59:56.632Z","steps":["trace[1314604589] 'process raft request'  (duration: 130.872303ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:00:35.067Z","caller":"traceutil/trace.go:171","msg":"trace[1281651206] transaction","detail":"{read_only:false; response_revision:143481; number_of_response:1; }","duration":"170.506595ms","start":"2023-08-18T02:00:34.896Z","end":"2023-08-18T02:00:35.067Z","steps":["trace[1281651206] 'process raft request'  (duration: 170.016695ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:00:35.703Z","caller":"traceutil/trace.go:171","msg":"trace[1157217335] linearizableReadLoop","detail":"{readStateIndex:180655; appliedIndex:180654; }","duration":"184.344896ms","start":"2023-08-18T02:00:35.519Z","end":"2023-08-18T02:00:35.703Z","steps":["trace[1157217335] 'read index received'  (duration: 183.673296ms)","trace[1157217335] 'applied index is now lower than readState.Index'  (duration: 668.8¬µs)"],"step_count":2}
{"level":"info","ts":"2023-08-18T02:00:35.704Z","caller":"traceutil/trace.go:171","msg":"trace[360890630] transaction","detail":"{read_only:false; response_revision:143482; number_of_response:1; }","duration":"318.300494ms","start":"2023-08-18T02:00:35.386Z","end":"2023-08-18T02:00:35.704Z","steps":["trace[360890630] 'process raft request'  (duration: 317.332394ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:35.704Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.215896ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:604"}
{"level":"info","ts":"2023-08-18T02:00:35.705Z","caller":"traceutil/trace.go:171","msg":"trace[209949127] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:143482; }","duration":"185.578696ms","start":"2023-08-18T02:00:35.519Z","end":"2023-08-18T02:00:35.705Z","steps":["trace[209949127] 'agreement among raft nodes before linearized reading'  (duration: 184.961196ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:35.704Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T02:00:35.386Z","time spent":"318.521194ms","remote":"127.0.0.1:53696","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:143474 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2023-08-18T02:00:35.935Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.854298ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128023100508553793 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:143480 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2023-08-18T02:00:35.935Z","caller":"traceutil/trace.go:171","msg":"trace[2017467732] transaction","detail":"{read_only:false; response_revision:143483; number_of_response:1; }","duration":"217.289696ms","start":"2023-08-18T02:00:35.717Z","end":"2023-08-18T02:00:35.935Z","steps":["trace[2017467732] 'process raft request'  (duration: 110.255798ms)","trace[2017467732] 'compare'  (duration: 106.771698ms)"],"step_count":2}
{"level":"info","ts":"2023-08-18T02:00:38.298Z","caller":"traceutil/trace.go:171","msg":"trace[651817131] transaction","detail":"{read_only:false; response_revision:143484; number_of_response:1; }","duration":"356.696893ms","start":"2023-08-18T02:00:37.942Z","end":"2023-08-18T02:00:38.298Z","steps":["trace[651817131] 'process raft request'  (duration: 356.130093ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:38.299Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T02:00:37.942Z","time spent":"357.007993ms","remote":"127.0.0.1:53604","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:143483 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-08-18T02:00:38.726Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.790896ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:340"}
{"level":"info","ts":"2023-08-18T02:00:38.726Z","caller":"traceutil/trace.go:171","msg":"trace[163633698] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:143484; }","duration":"180.987996ms","start":"2023-08-18T02:00:38.545Z","end":"2023-08-18T02:00:38.726Z","steps":["trace[163633698] 'range keys from in-memory index tree'  (duration: 180.585296ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:00:40.759Z","caller":"traceutil/trace.go:171","msg":"trace[873259617] transaction","detail":"{read_only:false; response_revision:143486; number_of_response:1; }","duration":"447.820991ms","start":"2023-08-18T02:00:40.312Z","end":"2023-08-18T02:00:40.759Z","steps":["trace[873259617] 'process raft request'  (duration: 447.591191ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:40.760Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T02:00:40.312Z","time spent":"447.973791ms","remote":"127.0.0.1:53604","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:143484 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-08-18T02:00:40.973Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"134.541597ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-08-18T02:00:40.973Z","caller":"traceutil/trace.go:171","msg":"trace[488262773] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:143486; }","duration":"134.955697ms","start":"2023-08-18T02:00:40.838Z","end":"2023-08-18T02:00:40.973Z","steps":["trace[488262773] 'range keys from in-memory index tree'  (duration: 134.300197ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:43.323Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128023100508553820,"retry-timeout":"500ms"}
{"level":"info","ts":"2023-08-18T02:00:43.434Z","caller":"traceutil/trace.go:171","msg":"trace[406830821] linearizableReadLoop","detail":"{readStateIndex:180661; appliedIndex:180660; }","duration":"612.157692ms","start":"2023-08-18T02:00:42.822Z","end":"2023-08-18T02:00:43.434Z","steps":["trace[406830821] 'read index received'  (duration: 611.926492ms)","trace[406830821] 'applied index is now lower than readState.Index'  (duration: 227.9¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-08-18T02:00:43.435Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"612.813092ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-08-18T02:00:43.435Z","caller":"traceutil/trace.go:171","msg":"trace[862318211] transaction","detail":"{read_only:false; response_revision:143487; number_of_response:1; }","duration":"669.00339ms","start":"2023-08-18T02:00:42.766Z","end":"2023-08-18T02:00:43.435Z","steps":["trace[862318211] 'process raft request'  (duration: 668.05039ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:00:43.435Z","caller":"traceutil/trace.go:171","msg":"trace[135242962] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:143487; }","duration":"612.957592ms","start":"2023-08-18T02:00:42.822Z","end":"2023-08-18T02:00:43.435Z","steps":["trace[135242962] 'agreement among raft nodes before linearized reading'  (duration: 612.635692ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:43.435Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T02:00:42.822Z","time spent":"613.089192ms","remote":"127.0.0.1:53456","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-08-18T02:00:43.435Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T02:00:42.766Z","time spent":"669.30449ms","remote":"127.0.0.1:53604","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:143486 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-08-18T02:00:45.805Z","caller":"traceutil/trace.go:171","msg":"trace[1621574441] linearizableReadLoop","detail":"{readStateIndex:180663; appliedIndex:180662; }","duration":"363.439498ms","start":"2023-08-18T02:00:45.442Z","end":"2023-08-18T02:00:45.805Z","steps":["trace[1621574441] 'read index received'  (duration: 362.746998ms)","trace[1621574441] 'applied index is now lower than readState.Index'  (duration: 689.9¬µs)"],"step_count":2}
{"level":"info","ts":"2023-08-18T02:00:45.806Z","caller":"traceutil/trace.go:171","msg":"trace[1887179456] transaction","detail":"{read_only:false; response_revision:143488; number_of_response:1; }","duration":"408.574397ms","start":"2023-08-18T02:00:45.397Z","end":"2023-08-18T02:00:45.806Z","steps":["trace[1887179456] 'process raft request'  (duration: 407.817297ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:45.806Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T02:00:45.397Z","time spent":"408.747397ms","remote":"127.0.0.1:53696","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:143481 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2023-08-18T02:00:45.806Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"363.929298ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:604"}
{"level":"info","ts":"2023-08-18T02:00:45.806Z","caller":"traceutil/trace.go:171","msg":"trace[1847859427] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:143488; }","duration":"364.071398ms","start":"2023-08-18T02:00:45.442Z","end":"2023-08-18T02:00:45.806Z","steps":["trace[1847859427] 'agreement among raft nodes before linearized reading'  (duration: 363.760998ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:00:45.806Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-18T02:00:45.442Z","time spent":"364.295798ms","remote":"127.0.0.1:53604","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":628,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2023-08-18T02:00:56.252Z","caller":"traceutil/trace.go:171","msg":"trace[1612938330] transaction","detail":"{read_only:false; response_revision:143497; number_of_response:1; }","duration":"110.021899ms","start":"2023-08-18T02:00:56.142Z","end":"2023-08-18T02:00:56.252Z","steps":["trace[1612938330] 'process raft request'  (duration: 109.440099ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:00:56.278Z","caller":"traceutil/trace.go:171","msg":"trace[1849001646] transaction","detail":"{read_only:false; response_revision:143498; number_of_response:1; }","duration":"100.694199ms","start":"2023-08-18T02:00:56.177Z","end":"2023-08-18T02:00:56.278Z","steps":["trace[1849001646] 'process raft request'  (duration: 100.506499ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:01:39.636Z","caller":"traceutil/trace.go:171","msg":"trace[820467679] transaction","detail":"{read_only:false; response_revision:143532; number_of_response:1; }","duration":"123.366499ms","start":"2023-08-18T02:01:39.513Z","end":"2023-08-18T02:01:39.636Z","steps":["trace[820467679] 'process raft request'  (duration: 123.182199ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:01:54.188Z","caller":"traceutil/trace.go:171","msg":"trace[1113076525] transaction","detail":"{read_only:false; response_revision:143542; number_of_response:1; }","duration":"107.418999ms","start":"2023-08-18T02:01:54.081Z","end":"2023-08-18T02:01:54.188Z","steps":["trace[1113076525] 'process raft request'  (duration: 52.683799ms)","trace[1113076525] 'compare'  (duration: 54.5819ms)"],"step_count":2}
{"level":"info","ts":"2023-08-18T02:02:08.131Z","caller":"traceutil/trace.go:171","msg":"trace[2054731184] transaction","detail":"{read_only:false; response_revision:143552; number_of_response:1; }","duration":"100.948498ms","start":"2023-08-18T02:02:08.030Z","end":"2023-08-18T02:02:08.131Z","steps":["trace[2054731184] 'process raft request'  (duration: 100.835098ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:02:12.921Z","caller":"traceutil/trace.go:171","msg":"trace[883638002] transaction","detail":"{read_only:false; response_revision:143557; number_of_response:1; }","duration":"106.078797ms","start":"2023-08-18T02:02:12.815Z","end":"2023-08-18T02:02:12.921Z","steps":["trace[883638002] 'process raft request'  (duration: 105.655097ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:02:18.765Z","caller":"traceutil/trace.go:171","msg":"trace[14124999] transaction","detail":"{read_only:false; response_revision:143562; number_of_response:1; }","duration":"100.145297ms","start":"2023-08-18T02:02:18.665Z","end":"2023-08-18T02:02:18.765Z","steps":["trace[14124999] 'process raft request'  (duration: 99.988897ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:02:31.517Z","caller":"traceutil/trace.go:171","msg":"trace[1249136881] transaction","detail":"{read_only:false; response_revision:143572; number_of_response:1; }","duration":"119.052699ms","start":"2023-08-18T02:02:31.397Z","end":"2023-08-18T02:02:31.516Z","steps":["trace[1249136881] 'process raft request'  (duration: 118.813699ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:02:54.346Z","caller":"traceutil/trace.go:171","msg":"trace[739883974] transaction","detail":"{read_only:false; response_revision:143589; number_of_response:1; }","duration":"102.075996ms","start":"2023-08-18T02:02:54.244Z","end":"2023-08-18T02:02:54.346Z","steps":["trace[739883974] 'process raft request'  (duration: 101.935696ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-18T02:03:18.810Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"143.505594ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:480"}
{"level":"info","ts":"2023-08-18T02:03:18.810Z","caller":"traceutil/trace.go:171","msg":"trace[1552518776] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:143607; }","duration":"143.663094ms","start":"2023-08-18T02:03:18.666Z","end":"2023-08-18T02:03:18.810Z","steps":["trace[1552518776] 'range keys from in-memory index tree'  (duration: 143.112094ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:03:19.225Z","caller":"traceutil/trace.go:171","msg":"trace[101527775] transaction","detail":"{read_only:false; response_revision:143608; number_of_response:1; }","duration":"206.767491ms","start":"2023-08-18T02:03:19.018Z","end":"2023-08-18T02:03:19.225Z","steps":["trace[101527775] 'process raft request'  (duration: 206.615491ms)"],"step_count":1}
{"level":"info","ts":"2023-08-18T02:03:23.427Z","caller":"traceutil/trace.go:171","msg":"trace[1000010276] transaction","detail":"{read_only:false; response_revision:143612; number_of_response:1; }","duration":"103.345496ms","start":"2023-08-18T02:03:23.324Z","end":"2023-08-18T02:03:23.427Z","steps":["trace[1000010276] 'process raft request'  (duration: 103.211996ms)"],"step_count":1}

* 
* ==> etcd [c384b393219c] <==
* {"level":"info","ts":"2023-08-20T07:39:47.581Z","caller":"traceutil/trace.go:171","msg":"trace[1110185717] transaction","detail":"{read_only:false; response_revision:145992; number_of_response:1; }","duration":"118.316415ms","start":"2023-08-20T07:39:47.463Z","end":"2023-08-20T07:39:47.581Z","steps":["trace[1110185717] 'process raft request'  (duration: 118.161015ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:39:52.695Z","caller":"traceutil/trace.go:171","msg":"trace[1416056219] transaction","detail":"{read_only:false; response_revision:145996; number_of_response:1; }","duration":"108.614614ms","start":"2023-08-20T07:39:52.587Z","end":"2023-08-20T07:39:52.695Z","steps":["trace[1416056219] 'process raft request'  (duration: 108.416514ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:40:24.943Z","caller":"traceutil/trace.go:171","msg":"trace[1565536469] transaction","detail":"{read_only:false; response_revision:146022; number_of_response:1; }","duration":"138.750304ms","start":"2023-08-20T07:40:24.804Z","end":"2023-08-20T07:40:24.943Z","steps":["trace[1565536469] 'process raft request'  (duration: 138.523504ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:40:29.152Z","caller":"traceutil/trace.go:171","msg":"trace[1850541376] transaction","detail":"{read_only:false; response_revision:146024; number_of_response:1; }","duration":"119.297902ms","start":"2023-08-20T07:40:29.033Z","end":"2023-08-20T07:40:29.152Z","steps":["trace[1850541376] 'process raft request'  (duration: 119.164102ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:40:34.864Z","caller":"traceutil/trace.go:171","msg":"trace[1641109348] transaction","detail":"{read_only:false; response_revision:146029; number_of_response:1; }","duration":"147.091599ms","start":"2023-08-20T07:40:34.717Z","end":"2023-08-20T07:40:34.864Z","steps":["trace[1641109348] 'process raft request'  (duration: 146.930699ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:40:47.856Z","caller":"traceutil/trace.go:171","msg":"trace[234697083] transaction","detail":"{read_only:false; response_revision:146039; number_of_response:1; }","duration":"116.621398ms","start":"2023-08-20T07:40:47.739Z","end":"2023-08-20T07:40:47.856Z","steps":["trace[234697083] 'process raft request'  (duration: 116.459898ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:41:06.597Z","caller":"traceutil/trace.go:171","msg":"trace[1887020480] transaction","detail":"{read_only:false; response_revision:146054; number_of_response:1; }","duration":"109.424494ms","start":"2023-08-20T07:41:06.488Z","end":"2023-08-20T07:41:06.597Z","steps":["trace[1887020480] 'process raft request'  (duration: 109.272194ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:41:39.667Z","caller":"traceutil/trace.go:171","msg":"trace[430759962] transaction","detail":"{read_only:false; response_revision:146079; number_of_response:1; }","duration":"100.150399ms","start":"2023-08-20T07:41:39.567Z","end":"2023-08-20T07:41:39.667Z","steps":["trace[430759962] 'process raft request'  (duration: 100.035699ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-20T07:41:45.982Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"255.685497ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-08-20T07:41:45.982Z","caller":"traceutil/trace.go:171","msg":"trace[1094626186] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:146083; }","duration":"255.831197ms","start":"2023-08-20T07:41:45.726Z","end":"2023-08-20T07:41:45.982Z","steps":["trace[1094626186] 'range keys from in-memory index tree'  (duration: 255.525797ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:41:46.120Z","caller":"traceutil/trace.go:171","msg":"trace[505036723] transaction","detail":"{read_only:false; response_revision:146084; number_of_response:1; }","duration":"132.509398ms","start":"2023-08-20T07:41:45.988Z","end":"2023-08-20T07:41:46.120Z","steps":["trace[505036723] 'process raft request'  (duration: 132.382698ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:41:58.573Z","caller":"traceutil/trace.go:171","msg":"trace[46406688] transaction","detail":"{read_only:false; response_revision:146094; number_of_response:1; }","duration":"122.639999ms","start":"2023-08-20T07:41:58.451Z","end":"2023-08-20T07:41:58.573Z","steps":["trace[46406688] 'process raft request'  (duration: 122.492099ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:41:58.701Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":145859}
{"level":"info","ts":"2023-08-20T07:41:58.702Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":145859,"took":"874.1¬µs","hash":3844224588}
{"level":"info","ts":"2023-08-20T07:41:58.702Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3844224588,"revision":145859,"compact-revision":145621}
{"level":"info","ts":"2023-08-20T07:42:02.783Z","caller":"traceutil/trace.go:171","msg":"trace[195367481] transaction","detail":"{read_only:false; response_revision:146098; number_of_response:1; }","duration":"120.672199ms","start":"2023-08-20T07:42:02.663Z","end":"2023-08-20T07:42:02.783Z","steps":["trace[195367481] 'process raft request'  (duration: 120.363799ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:42:21.948Z","caller":"traceutil/trace.go:171","msg":"trace[1419006827] transaction","detail":"{read_only:false; response_revision:146113; number_of_response:1; }","duration":"108.772099ms","start":"2023-08-20T07:42:21.839Z","end":"2023-08-20T07:42:21.948Z","steps":["trace[1419006827] 'process raft request'  (duration: 64.429199ms)","trace[1419006827] 'compare'  (duration: 44.2169ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:42:31.797Z","caller":"traceutil/trace.go:171","msg":"trace[1417949593] transaction","detail":"{read_only:false; response_revision:146120; number_of_response:1; }","duration":"113.644699ms","start":"2023-08-20T07:42:31.684Z","end":"2023-08-20T07:42:31.797Z","steps":["trace[1417949593] 'process raft request'  (duration: 113.438099ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:42:35.978Z","caller":"traceutil/trace.go:171","msg":"trace[1442140149] transaction","detail":"{read_only:false; response_revision:146123; number_of_response:1; }","duration":"109.915497ms","start":"2023-08-20T07:42:35.868Z","end":"2023-08-20T07:42:35.978Z","steps":["trace[1442140149] 'process raft request'  (duration: 109.766197ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:42:40.119Z","caller":"traceutil/trace.go:171","msg":"trace[427091617] transaction","detail":"{read_only:false; response_revision:146127; number_of_response:1; }","duration":"106.924998ms","start":"2023-08-20T07:42:40.012Z","end":"2023-08-20T07:42:40.119Z","steps":["trace[427091617] 'process raft request'  (duration: 106.786198ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:42:49.574Z","caller":"traceutil/trace.go:171","msg":"trace[1956052635] transaction","detail":"{read_only:false; response_revision:146135; number_of_response:1; }","duration":"113.004ms","start":"2023-08-20T07:42:49.461Z","end":"2023-08-20T07:42:49.574Z","steps":["trace[1956052635] 'process raft request'  (duration: 112.8727ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-20T07:43:00.848Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.873492ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/servicenodeports\" ","response":"range_response_count:1 size:369"}
{"level":"info","ts":"2023-08-20T07:43:00.848Z","caller":"traceutil/trace.go:171","msg":"trace[1675901746] range","detail":"{range_begin:/registry/ranges/servicenodeports; range_end:; response_count:1; response_revision:146144; }","duration":"110.035092ms","start":"2023-08-20T07:43:00.738Z","end":"2023-08-20T07:43:00.848Z","steps":["trace[1675901746] 'range keys from in-memory index tree'  (duration: 109.743792ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:43:02.806Z","caller":"traceutil/trace.go:171","msg":"trace[1455383084] transaction","detail":"{read_only:false; response_revision:146146; number_of_response:1; }","duration":"102.903693ms","start":"2023-08-20T07:43:02.703Z","end":"2023-08-20T07:43:02.806Z","steps":["trace[1455383084] 'process raft request'  (duration: 102.711293ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:43:23.380Z","caller":"traceutil/trace.go:171","msg":"trace[1149482923] transaction","detail":"{read_only:false; response_revision:146162; number_of_response:1; }","duration":"105.9371ms","start":"2023-08-20T07:43:23.274Z","end":"2023-08-20T07:43:23.380Z","steps":["trace[1149482923] 'process raft request'  (duration: 105.7491ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:43:35.836Z","caller":"traceutil/trace.go:171","msg":"trace[1421885460] transaction","detail":"{read_only:false; response_revision:146171; number_of_response:1; }","duration":"106.128005ms","start":"2023-08-20T07:43:35.730Z","end":"2023-08-20T07:43:35.836Z","steps":["trace[1421885460] 'process raft request'  (duration: 105.932705ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:43:40.025Z","caller":"traceutil/trace.go:171","msg":"trace[1561575748] transaction","detail":"{read_only:false; response_revision:146175; number_of_response:1; }","duration":"100.923599ms","start":"2023-08-20T07:43:39.924Z","end":"2023-08-20T07:43:40.025Z","steps":["trace[1561575748] 'process raft request'  (duration: 100.760499ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:43:49.867Z","caller":"traceutil/trace.go:171","msg":"trace[1440677991] transaction","detail":"{read_only:false; response_revision:146182; number_of_response:1; }","duration":"110.237501ms","start":"2023-08-20T07:43:49.757Z","end":"2023-08-20T07:43:49.867Z","steps":["trace[1440677991] 'process raft request'  (duration: 110.073201ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:43:54.484Z","caller":"traceutil/trace.go:171","msg":"trace[364136514] transaction","detail":"{read_only:false; response_revision:146186; number_of_response:1; }","duration":"127.404002ms","start":"2023-08-20T07:43:54.357Z","end":"2023-08-20T07:43:54.484Z","steps":["trace[364136514] 'process raft request'  (duration: 127.223002ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:43:58.706Z","caller":"traceutil/trace.go:171","msg":"trace[1646067477] transaction","detail":"{read_only:false; response_revision:146189; number_of_response:1; }","duration":"101.726901ms","start":"2023-08-20T07:43:58.605Z","end":"2023-08-20T07:43:58.706Z","steps":["trace[1646067477] 'process raft request'  (duration: 87.850601ms)","trace[1646067477] 'compare'  (duration: 13.7856ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:44:02.804Z","caller":"traceutil/trace.go:171","msg":"trace[1030942480] transaction","detail":"{read_only:false; response_revision:146193; number_of_response:1; }","duration":"103.601211ms","start":"2023-08-20T07:44:02.701Z","end":"2023-08-20T07:44:02.804Z","steps":["trace[1030942480] 'process raft request'  (duration: 103.062911ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:44:04.980Z","caller":"traceutil/trace.go:171","msg":"trace[2062289521] transaction","detail":"{read_only:false; response_revision:146194; number_of_response:1; }","duration":"159.042817ms","start":"2023-08-20T07:44:04.821Z","end":"2023-08-20T07:44:04.980Z","steps":["trace[2062289521] 'process raft request'  (duration: 158.912117ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:44:31.975Z","caller":"traceutil/trace.go:171","msg":"trace[998573673] transaction","detail":"{read_only:false; response_revision:146216; number_of_response:1; }","duration":"104.173701ms","start":"2023-08-20T07:44:31.871Z","end":"2023-08-20T07:44:31.975Z","steps":["trace[998573673] 'process raft request'  (duration: 103.896801ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:45:42.025Z","caller":"traceutil/trace.go:171","msg":"trace[1851005015] transaction","detail":"{read_only:false; response_revision:146270; number_of_response:1; }","duration":"120.535593ms","start":"2023-08-20T07:45:41.904Z","end":"2023-08-20T07:45:42.025Z","steps":["trace[1851005015] 'process raft request'  (duration: 120.239793ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:46:13.800Z","caller":"traceutil/trace.go:171","msg":"trace[1180027140] transaction","detail":"{read_only:false; response_revision:146295; number_of_response:1; }","duration":"112.785999ms","start":"2023-08-20T07:46:13.687Z","end":"2023-08-20T07:46:13.800Z","steps":["trace[1180027140] 'process raft request'  (duration: 112.579899ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:46:31.981Z","caller":"traceutil/trace.go:171","msg":"trace[557867083] transaction","detail":"{read_only:false; response_revision:146308; number_of_response:1; }","duration":"118.252702ms","start":"2023-08-20T07:46:31.862Z","end":"2023-08-20T07:46:31.981Z","steps":["trace[557867083] 'process raft request'  (duration: 57.465101ms)","trace[557867083] 'compare'  (duration: 60.448601ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:46:33.926Z","caller":"traceutil/trace.go:171","msg":"trace[1456003545] transaction","detail":"{read_only:false; response_revision:146310; number_of_response:1; }","duration":"113.2002ms","start":"2023-08-20T07:46:33.812Z","end":"2023-08-20T07:46:33.926Z","steps":["trace[1456003545] 'process raft request'  (duration: 113.0075ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-20T07:46:48.666Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"173.2088ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-08-20T07:46:48.666Z","caller":"traceutil/trace.go:171","msg":"trace[1240161147] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:146321; }","duration":"173.365ms","start":"2023-08-20T07:46:48.492Z","end":"2023-08-20T07:46:48.666Z","steps":["trace[1240161147] 'count revisions from in-memory index tree'  (duration: 173.1103ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:46:56.740Z","caller":"traceutil/trace.go:171","msg":"trace[1630322404] transaction","detail":"{read_only:false; response_revision:146328; number_of_response:1; }","duration":"120.801101ms","start":"2023-08-20T07:46:56.619Z","end":"2023-08-20T07:46:56.740Z","steps":["trace[1630322404] 'process raft request'  (duration: 120.604601ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:46:58.864Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":146095}
{"level":"info","ts":"2023-08-20T07:46:58.865Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":146095,"took":"819.8¬µs","hash":488357411}
{"level":"info","ts":"2023-08-20T07:46:58.865Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":488357411,"revision":146095,"compact-revision":145859}
{"level":"info","ts":"2023-08-20T07:47:05.226Z","caller":"traceutil/trace.go:171","msg":"trace[991474198] transaction","detail":"{read_only:false; response_revision:146336; number_of_response:1; }","duration":"160.4655ms","start":"2023-08-20T07:47:05.065Z","end":"2023-08-20T07:47:05.226Z","steps":["trace[991474198] 'process raft request'  (duration: 94.8301ms)","trace[991474198] 'compare'  (duration: 65.4107ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:47:29.946Z","caller":"traceutil/trace.go:171","msg":"trace[1548454347] transaction","detail":"{read_only:false; response_revision:146354; number_of_response:1; }","duration":"112.533ms","start":"2023-08-20T07:47:29.833Z","end":"2023-08-20T07:47:29.946Z","steps":["trace[1548454347] 'process raft request'  (duration: 112.3891ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-20T07:47:32.083Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.5656ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-08-20T07:47:32.083Z","caller":"traceutil/trace.go:171","msg":"trace[274005285] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:146355; }","duration":"133.9209ms","start":"2023-08-20T07:47:31.949Z","end":"2023-08-20T07:47:32.083Z","steps":["trace[274005285] 'range keys from in-memory index tree'  (duration: 132.9098ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-20T07:47:32.083Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.2158ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-08-20T07:47:32.084Z","caller":"traceutil/trace.go:171","msg":"trace[1274500016] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:146355; }","duration":"121.4057ms","start":"2023-08-20T07:47:31.962Z","end":"2023-08-20T07:47:32.083Z","steps":["trace[1274500016] 'range keys from in-memory index tree'  (duration: 121.1363ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:47:38.372Z","caller":"traceutil/trace.go:171","msg":"trace[681761280] transaction","detail":"{read_only:false; response_revision:146361; number_of_response:1; }","duration":"108.670899ms","start":"2023-08-20T07:47:38.264Z","end":"2023-08-20T07:47:38.372Z","steps":["trace[681761280] 'process raft request'  (duration: 108.516299ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-20T07:47:42.034Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.7527ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128023236184836957 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:146355 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128023236184836955 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2023-08-20T07:47:42.034Z","caller":"traceutil/trace.go:171","msg":"trace[1899812820] transaction","detail":"{read_only:false; response_revision:146363; number_of_response:1; }","duration":"137.1905ms","start":"2023-08-20T07:47:41.897Z","end":"2023-08-20T07:47:42.034Z","steps":["trace[1899812820] 'process raft request'  (duration: 27.0919ms)","trace[1899812820] 'compare'  (duration: 109.658ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:47:52.019Z","caller":"traceutil/trace.go:171","msg":"trace[49353771] transaction","detail":"{read_only:false; response_revision:146371; number_of_response:1; }","duration":"113.5651ms","start":"2023-08-20T07:47:51.905Z","end":"2023-08-20T07:47:52.019Z","steps":["trace[49353771] 'process raft request'  (duration: 58.774ms)","trace[49353771] 'compare'  (duration: 54.6426ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:47:56.152Z","caller":"traceutil/trace.go:171","msg":"trace[715697304] transaction","detail":"{read_only:false; response_revision:146375; number_of_response:1; }","duration":"106.515205ms","start":"2023-08-20T07:47:56.046Z","end":"2023-08-20T07:47:56.152Z","steps":["trace[715697304] 'process raft request'  (duration: 106.363105ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:47:56.717Z","caller":"traceutil/trace.go:171","msg":"trace[669545231] transaction","detail":"{read_only:false; response_revision:146376; number_of_response:1; }","duration":"115.578305ms","start":"2023-08-20T07:47:56.601Z","end":"2023-08-20T07:47:56.717Z","steps":["trace[669545231] 'process raft request'  (duration: 115.406405ms)"],"step_count":1}
{"level":"info","ts":"2023-08-20T07:48:17.034Z","caller":"traceutil/trace.go:171","msg":"trace[1194903981] transaction","detail":"{read_only:false; response_revision:146392; number_of_response:1; }","duration":"109.701403ms","start":"2023-08-20T07:48:16.924Z","end":"2023-08-20T07:48:17.034Z","steps":["trace[1194903981] 'process raft request'  (duration: 60.449002ms)","trace[1194903981] 'compare'  (duration: 49.004001ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:48:32.069Z","caller":"traceutil/trace.go:171","msg":"trace[1673141287] linearizableReadLoop","detail":"{readStateIndex:184319; appliedIndex:184318; }","duration":"180.731125ms","start":"2023-08-20T07:48:31.888Z","end":"2023-08-20T07:48:32.069Z","steps":["trace[1673141287] 'read index received'  (duration: 89.536912ms)","trace[1673141287] 'applied index is now lower than readState.Index'  (duration: 91.193413ms)"],"step_count":2}
{"level":"info","ts":"2023-08-20T07:48:32.069Z","caller":"traceutil/trace.go:171","msg":"trace[1964644383] transaction","detail":"{read_only:false; response_revision:146403; number_of_response:1; }","duration":"182.567125ms","start":"2023-08-20T07:48:31.886Z","end":"2023-08-20T07:48:32.069Z","steps":["trace[1964644383] 'process raft request'  (duration: 91.320912ms)","trace[1964644383] 'compare'  (duration: 91.062113ms)"],"step_count":2}
{"level":"warn","ts":"2023-08-20T07:48:32.069Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.921925ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-08-20T07:48:32.069Z","caller":"traceutil/trace.go:171","msg":"trace[635817039] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:146403; }","duration":"180.996425ms","start":"2023-08-20T07:48:31.888Z","end":"2023-08-20T07:48:32.069Z","steps":["trace[635817039] 'agreement among raft nodes before linearized reading'  (duration: 180.830725ms)"],"step_count":1}

* 
* ==> kernel <==
*  07:48:40 up  1:46,  0 users,  load average: 0.47, 0.54, 0.59
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [62ce3e4f4b44] <==
* I0820 06:54:13.130307       1 trace.go:219] Trace[220962379]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (20-Aug-2023 06:54:11.694) (total time: 1202ms):
Trace[220962379]: ---"initial value restored" 120ms (06:54:11.814)
Trace[220962379]: ---"Transaction prepared" 690ms (06:54:12.505)
Trace[220962379]: ---"Txn call completed" 391ms (06:54:12.896)
Trace[220962379]: [1.2023161s] [1.2023161s] END
I0820 06:54:13.921111       1 trace.go:219] Trace[142599836]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7bd590b6-012e-4be0-a9ae-f4a125cec72b,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.177d04ebad083cd3,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PATCH (20-Aug-2023 06:54:11.352) (total time: 2568ms):
Trace[142599836]: ---"limitedReadBody succeeded" len:125 30ms (06:54:11.383)
Trace[142599836]: ["GuaranteedUpdate etcd3" audit-id:7bd590b6-012e-4be0-a9ae-f4a125cec72b,key:/events/kube-system/kube-apiserver-minikube.177d04ebad083cd3,type:*core.Event,resource:events 2537ms (06:54:11.383)
Trace[142599836]:  ---"initial value restored" 81ms (06:54:11.465)
Trace[142599836]:  ---"About to Encode" 1190ms (06:54:12.655)
Trace[142599836]:  ---"Txn call completed" 1208ms (06:54:13.863)]
Trace[142599836]: ---"About to check admission control" 1080ms (06:54:12.545)
Trace[142599836]: ---"Object stored in database" 1318ms (06:54:13.863)
Trace[142599836]: ---"Writing http response done" 57ms (06:54:13.921)
Trace[142599836]: [2.568344673s] [2.568344673s] END
I0820 06:54:14.302555       1 trace.go:219] Trace[230362493]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ac97fc5d-64b1-4cad-b2dc-dceade997e4c,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (20-Aug-2023 06:54:13.230) (total time: 1072ms):
Trace[230362493]: ---"About to write a response" 1072ms (06:54:14.302)
Trace[230362493]: [1.072466254s] [1.072466254s] END
I0820 06:54:15.380500       1 trace.go:219] Trace[345510055]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (20-Aug-2023 06:54:14.601) (total time: 593ms):
Trace[345510055]: [593.198529ms] [593.198529ms] END
I0820 06:54:15.875469       1 trace.go:219] Trace[926105734]: "Get" accept:application/json, */*,audit-id:28d353dc-8a78-4bdd-a5c8-b8d0a3193123,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (20-Aug-2023 06:54:14.363) (total time: 1024ms):
Trace[926105734]: ---"About to write a response" 759ms (06:54:15.123)
Trace[926105734]: ---"Writing http response done" 265ms (06:54:15.388)
Trace[926105734]: [1.024588951s] [1.024588951s] END
I0820 06:54:15.922203       1 trace.go:219] Trace[1967444883]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:ede2cc77-30c8-43b6-99d8-735424657ec8,client:127.0.0.1,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (20-Aug-2023 06:54:15.381) (total time: 540ms):
Trace[1967444883]: ---"About to write a response" 540ms (06:54:15.921)
Trace[1967444883]: [540.949127ms] [540.949127ms] END
I0820 06:54:16.884151       1 trace.go:219] Trace[34605497]: "Update" accept:application/json, */*,audit-id:e746e0a8-ce00-4091-a2e9-b1d58f3ffd3b,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Aug-2023 06:54:15.921) (total time: 962ms):
Trace[34605497]: ---"limitedReadBody succeeded" len:1355 22ms (06:54:15.943)
Trace[34605497]: ---"Conversion done" 209ms (06:54:16.152)
Trace[34605497]: ["GuaranteedUpdate etcd3" audit-id:e746e0a8-ce00-4091-a2e9-b1d58f3ffd3b,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 625ms (06:54:16.258)
Trace[34605497]:  ---"About to Encode" 236ms (06:54:16.511)
Trace[34605497]:  ---"Txn call completed" 372ms (06:54:16.883)]
Trace[34605497]: [962.901648ms] [962.901648ms] END
I0820 06:54:19.902501       1 trace.go:219] Trace[2030303106]: "Update" accept:application/json, */*,audit-id:f306376d-9857-4945-bae9-69f8ff649199,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Aug-2023 06:54:19.318) (total time: 584ms):
Trace[2030303106]: ---"Conversion done" 262ms (06:54:19.581)
Trace[2030303106]: [584.105829ms] [584.105829ms] END
I0820 06:54:52.519931       1 trace.go:219] Trace[1518651967]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1df88714-0791-47f9-8ff0-5e6764457823,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/kubernetes,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (20-Aug-2023 06:54:51.489) (total time: 1030ms):
Trace[1518651967]: ---"Writing http response done" 1029ms (06:54:52.519)
Trace[1518651967]: [1.030495216s] [1.030495216s] END
I0820 06:54:53.283258       1 trace.go:219] Trace[675208707]: "Update" accept:application/json, */*,audit-id:d4caba64-8052-427b-8323-17b175d48c19,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Aug-2023 06:54:52.727) (total time: 550ms):
Trace[675208707]: ---"Conversion done" 39ms (06:54:52.767)
Trace[675208707]: ---"Writing http response done" 71ms (06:54:53.277)
Trace[675208707]: [550.257608ms] [550.257608ms] END
I0820 07:01:31.444371       1 trace.go:219] Trace[1928536214]: "Update" accept:application/json, */*,audit-id:8bc251b7-3498-49fb-ae47-0b19e5cb4338,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Aug-2023 07:01:30.931) (total time: 513ms):
Trace[1928536214]: ["GuaranteedUpdate etcd3" audit-id:8bc251b7-3498-49fb-ae47-0b19e5cb4338,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 512ms (07:01:30.931)
Trace[1928536214]:  ---"Txn call completed" 511ms (07:01:31.444)]
Trace[1928536214]: [513.0669ms] [513.0669ms] END
I0820 07:11:15.844209       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0820 07:11:36.657231       1 trace.go:219] Trace[879449167]: "Update" accept:application/json, */*,audit-id:abac60ac-aefc-4b9d-8936-a9e546a6b93b,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Aug-2023 07:11:36.135) (total time: 521ms):
Trace[879449167]: ["GuaranteedUpdate etcd3" audit-id:abac60ac-aefc-4b9d-8936-a9e546a6b93b,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 521ms (07:11:36.136)
Trace[879449167]:  ---"Txn call completed" 520ms (07:11:36.656)]
Trace[879449167]: [521.364001ms] [521.364001ms] END
I0820 07:11:43.779995       1 trace.go:219] Trace[1510188607]: "Get" accept:application/json, */*,audit-id:e6d4e4a4-b77c-4d8e-84b5-f36bf1a44d95,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (20-Aug-2023 07:11:42.841) (total time: 938ms):
Trace[1510188607]: ---"About to write a response" 938ms (07:11:43.779)
Trace[1510188607]: [938.761275ms] [938.761275ms] END
I0820 07:13:11.725002       1 trace.go:219] Trace[2029977131]: "Update" accept:application/json, */*,audit-id:042a7480-7f0d-447b-9bf6-96c6776dacbb,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (20-Aug-2023 07:13:11.177) (total time: 547ms):
Trace[2029977131]: ["GuaranteedUpdate etcd3" audit-id:042a7480-7f0d-447b-9bf6-96c6776dacbb,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 547ms (07:13:11.177)
Trace[2029977131]:  ---"Txn call completed" 546ms (07:13:11.724)]
Trace[2029977131]: [547.444569ms] [547.444569ms] END

* 
* ==> kube-apiserver [73d5528708d6] <==
* Trace[1841320021]:  ---"TransformToStorage succeeded" 946ms (01:51:36.005)
Trace[1841320021]:  ---"Txn call succeeded" 431ms (01:51:36.437)]
Trace[1841320021]: [1.37861949s] [1.37861949s] END
I0818 01:51:36.437855       1 trace.go:219] Trace[980064366]: "Update" accept:application/json, */*,audit-id:5ab9f95e-5d9c-4d71-a952-c9e9af80d6af,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Aug-2023 01:51:35.058) (total time: 1378ms):
Trace[980064366]: ["GuaranteedUpdate etcd3" audit-id:5ab9f95e-5d9c-4d71-a952-c9e9af80d6af,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1378ms (01:51:35.059)
Trace[980064366]:  ---"Txn call completed" 1377ms (01:51:36.437)]
Trace[980064366]: [1.37895399s] [1.37895399s] END
I0818 01:51:37.486193       1 trace.go:219] Trace[807521360]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (18-Aug-2023 01:51:35.059) (total time: 2426ms):
Trace[807521360]: ---"Transaction prepared" 1377ms (01:51:36.437)
Trace[807521360]: ---"Txn call completed" 1048ms (01:51:37.486)
Trace[807521360]: [2.426706283s] [2.426706283s] END
I0818 01:51:38.032005       1 trace.go:219] Trace[696436389]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (18-Aug-2023 01:51:37.488) (total time: 543ms):
Trace[696436389]: [543.034996ms] [543.034996ms] END
I0818 01:51:38.032225       1 trace.go:219] Trace[267607430]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:759a50aa-fdf5-47e4-a148-e7244af16ffa,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.177b22eb871e8d7e,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PATCH (18-Aug-2023 01:51:36.440) (total time: 1591ms):
Trace[267607430]: ["GuaranteedUpdate etcd3" audit-id:759a50aa-fdf5-47e4-a148-e7244af16ffa,key:/events/kube-system/kube-apiserver-minikube.177b22eb871e8d7e,type:*core.Event,resource:events 1591ms (01:51:36.440)
Trace[267607430]:  ---"initial value restored" 1045ms (01:51:37.486)
Trace[267607430]:  ---"Txn call completed" 544ms (01:51:38.031)]
Trace[267607430]: ---"Object stored in database" 544ms (01:51:38.031)
Trace[267607430]: [1.591839289s] [1.591839289s] END
I0818 01:51:41.632370       1 trace.go:219] Trace[1449056921]: "Update" accept:application/json, */*,audit-id:8ae31541-4c6b-4274-a418-8ccd660251a9,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Aug-2023 01:51:40.781) (total time: 851ms):
Trace[1449056921]: ["GuaranteedUpdate etcd3" audit-id:8ae31541-4c6b-4274-a418-8ccd660251a9,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 850ms (01:51:40.781)
Trace[1449056921]:  ---"Txn call completed" 850ms (01:51:41.632)]
Trace[1449056921]: [851.153094ms] [851.153094ms] END
E0818 01:51:43.626027       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0818 01:51:43.626207       1 authentication.go:70] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0818 01:51:47.026179       1 trace.go:219] Trace[1213289054]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (18-Aug-2023 01:51:44.100) (total time: 2925ms):
Trace[1213289054]: ---"Transaction prepared" 1158ms (01:51:45.260)
Trace[1213289054]: ---"Txn call completed" 1765ms (01:51:47.026)
Trace[1213289054]: [2.925670179s] [2.925670179s] END
I0818 01:51:48.337476       1 trace.go:219] Trace[369599694]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:61893a04-cc25-43b7-857e-3bd24e368009,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (18-Aug-2023 01:51:45.449) (total time: 2887ms):
Trace[369599694]: ["GuaranteedUpdate etcd3" audit-id:61893a04-cc25-43b7-857e-3bd24e368009,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 2887ms (01:51:45.449)
Trace[369599694]:  ---"Txn call completed" 2886ms (01:51:48.337)]
Trace[369599694]: [2.88790338s] [2.88790338s] END
I0818 01:51:48.337906       1 trace.go:219] Trace[187445336]: "Get" accept:application/json, */*,audit-id:2eb65c08-9ddd-45ff-99ef-795736ecc883,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (18-Aug-2023 01:51:46.100) (total time: 2236ms):
Trace[187445336]: ---"About to write a response" 2236ms (01:51:48.337)
Trace[187445336]: [2.236900384s] [2.236900384s] END
I0818 01:51:48.337931       1 trace.go:219] Trace[83558758]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4197dbf5-baa1-4945-975b-b38d1134cb9c,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:GET (18-Aug-2023 01:51:47.026) (total time: 1311ms):
Trace[83558758]: ---"About to write a response" 1311ms (01:51:48.337)
Trace[83558758]: [1.311168691s] [1.311168691s] END
I0818 01:51:48.339507       1 trace.go:219] Trace[220678860]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:1f02097e-bbb0-4e1c-9013-b8099096183f,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PUT (18-Aug-2023 01:51:45.411) (total time: 2928ms):
Trace[220678860]: ["GuaranteedUpdate etcd3" audit-id:1f02097e-bbb0-4e1c-9013-b8099096183f,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 2928ms (01:51:45.411)
Trace[220678860]:  ---"Txn call completed" 2927ms (01:51:48.339)]
Trace[220678860]: [2.92843668s] [2.92843668s] END
I0818 01:51:49.334347       1 trace.go:219] Trace[547097798]: "Update" accept:application/json, */*,audit-id:e5f08bbc-e5de-410d-a030-acf560ee5b61,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Aug-2023 01:51:48.339) (total time: 994ms):
Trace[547097798]: ["GuaranteedUpdate etcd3" audit-id:e5f08bbc-e5de-410d-a030-acf560ee5b61,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 994ms (01:51:48.339)
Trace[547097798]:  ---"Txn call completed" 993ms (01:51:49.334)]
Trace[547097798]: [994.714593ms] [994.714593ms] END
I0818 01:51:50.513372       1 trace.go:219] Trace[670232885]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f3846dda-79a1-4cf2-bed3-3fe7626dfe1e,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/kube-apiserver-minikube.177b22eb871e8d7e,user-agent:kubelet/v1.27.3 (linux/amd64) kubernetes/25b4e43,verb:PATCH (18-Aug-2023 01:51:48.340) (total time: 2172ms):
Trace[670232885]: ["GuaranteedUpdate etcd3" audit-id:f3846dda-79a1-4cf2-bed3-3fe7626dfe1e,key:/events/kube-system/kube-apiserver-minikube.177b22eb871e8d7e,type:*core.Event,resource:events 2172ms (01:51:48.341)
Trace[670232885]:  ---"initial value restored" 992ms (01:51:49.333)
Trace[670232885]:  ---"Txn call completed" 1178ms (01:51:50.513)]
Trace[670232885]: ---"Object stored in database" 1178ms (01:51:50.513)
Trace[670232885]: [2.172355385s] [2.172355385s] END
I0818 01:51:52.044835       1 trace.go:219] Trace[267144206]: "Get" accept:application/json, */*,audit-id:9e86c75d-c5ca-4a27-be63-c5a2da7651ee,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (18-Aug-2023 01:51:51.336) (total time: 708ms):
Trace[267144206]: ---"About to write a response" 708ms (01:51:52.044)
Trace[267144206]: [708.236195ms] [708.236195ms] END
I0818 02:00:43.437781       1 trace.go:219] Trace[1309219634]: "Update" accept:application/json, */*,audit-id:f52b1851-0bbd-4ba2-9bb3-82f3d5517dbd,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (18-Aug-2023 02:00:42.765) (total time: 672ms):
Trace[1309219634]: ["GuaranteedUpdate etcd3" audit-id:f52b1851-0bbd-4ba2-9bb3-82f3d5517dbd,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 672ms (02:00:42.765)
Trace[1309219634]:  ---"Txn call completed" 671ms (02:00:43.437)]
Trace[1309219634]: [672.35979ms] [672.35979ms] END

* 
* ==> kube-controller-manager [4289a42d67a1] <==
* I0820 06:52:13.868682       1 shared_informer.go:311] Waiting for caches to sync for token_cleaner
I0820 06:52:13.868691       1 shared_informer.go:318] Caches are synced for token_cleaner
E0820 06:52:13.968297       1 core.go:213] "Failed to start cloud node lifecycle controller" err="no cloud provider provided"
I0820 06:52:13.968335       1 controllermanager.go:616] "Warning: skipping controller" controller="cloud-node-lifecycle"
I0820 06:52:14.017697       1 controllermanager.go:638] "Started controller" controller="replicationcontroller"
I0820 06:52:14.017849       1 replica_set.go:201] "Starting controller" name="replicationcontroller"
I0820 06:52:14.017869       1 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0820 06:52:14.022281       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0820 06:52:14.103055       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0820 06:52:14.111140       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0820 06:52:14.138836       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0820 06:52:14.140012       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0820 06:52:14.141208       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0820 06:52:14.142385       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0820 06:52:14.149010       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0820 06:52:14.151390       1 shared_informer.go:318] Caches are synced for TTL
I0820 06:52:14.156141       1 shared_informer.go:318] Caches are synced for PV protection
I0820 06:52:14.156835       1 shared_informer.go:318] Caches are synced for expand
I0820 06:52:14.161933       1 shared_informer.go:318] Caches are synced for TTL after finished
I0820 06:52:14.167347       1 shared_informer.go:318] Caches are synced for node
I0820 06:52:14.167686       1 range_allocator.go:174] "Sending events to api server"
I0820 06:52:14.167792       1 range_allocator.go:178] "Starting range CIDR allocator"
I0820 06:52:14.167802       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0820 06:52:14.167812       1 shared_informer.go:318] Caches are synced for cidrallocator
I0820 06:52:14.173051       1 shared_informer.go:318] Caches are synced for service account
I0820 06:52:14.183217       1 shared_informer.go:318] Caches are synced for cronjob
I0820 06:52:14.186764       1 shared_informer.go:318] Caches are synced for namespace
I0820 06:52:14.189279       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0820 06:52:14.203565       1 shared_informer.go:318] Caches are synced for crt configmap
I0820 06:52:14.233356       1 shared_informer.go:318] Caches are synced for stateful set
I0820 06:52:14.234573       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0820 06:52:14.244301       1 shared_informer.go:318] Caches are synced for resource quota
I0820 06:52:14.246315       1 shared_informer.go:318] Caches are synced for deployment
I0820 06:52:14.252009       1 shared_informer.go:318] Caches are synced for disruption
I0820 06:52:14.262769       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0820 06:52:14.267536       1 shared_informer.go:318] Caches are synced for ephemeral
I0820 06:52:14.267583       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0820 06:52:14.268992       1 shared_informer.go:318] Caches are synced for job
I0820 06:52:14.277146       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0820 06:52:14.285762       1 shared_informer.go:318] Caches are synced for endpoint
I0820 06:52:14.294333       1 shared_informer.go:318] Caches are synced for taint
I0820 06:52:14.294534       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0820 06:52:14.294801       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0820 06:52:14.294565       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0820 06:52:14.294922       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0820 06:52:14.294995       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0820 06:52:14.295228       1 taint_manager.go:211] "Sending events to api server"
I0820 06:52:14.299335       1 shared_informer.go:318] Caches are synced for persistent volume
I0820 06:52:14.301952       1 shared_informer.go:318] Caches are synced for HPA
I0820 06:52:14.310420       1 shared_informer.go:318] Caches are synced for daemon sets
I0820 06:52:14.312896       1 shared_informer.go:318] Caches are synced for attach detach
I0820 06:52:14.336432       1 shared_informer.go:318] Caches are synced for PVC protection
I0820 06:52:14.353719       1 shared_informer.go:318] Caches are synced for resource quota
I0820 06:52:14.353815       1 shared_informer.go:318] Caches are synced for GC
I0820 06:52:14.353956       1 shared_informer.go:318] Caches are synced for ReplicationController
I0820 06:52:14.711960       1 shared_informer.go:318] Caches are synced for garbage collector
I0820 06:52:14.718546       1 shared_informer.go:318] Caches are synced for garbage collector
I0820 06:52:14.718624       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0820 07:11:15.971242       1 event.go:307] "Event occurred" object="default/nginx-depl" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set nginx-depl-6b7698588c to 1"
I0820 07:11:16.103246       1 event.go:307] "Event occurred" object="default/nginx-depl-6b7698588c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-depl-6b7698588c-n46d4"

* 
* ==> kube-controller-manager [c33ed70697aa] <==
* I0814 03:39:30.133360       1 shared_informer.go:318] Caches are synced for ephemeral
I0814 03:39:30.146668       1 shared_informer.go:318] Caches are synced for PVC protection
I0814 03:39:30.165094       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=[10.244.0.0/24]
I0814 03:39:30.180058       1 shared_informer.go:318] Caches are synced for attach detach
I0814 03:39:30.198953       1 shared_informer.go:318] Caches are synced for expand
I0814 03:39:30.570866       1 shared_informer.go:318] Caches are synced for garbage collector
I0814 03:39:30.570923       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0814 03:39:30.588520       1 shared_informer.go:318] Caches are synced for garbage collector
I0814 03:39:30.996457       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5d78c9869d to 1"
I0814 03:39:31.769301       1 event.go:307] "Event occurred" object="kube-system/coredns-5d78c9869d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5d78c9869d-9f9mw"
I0814 03:39:31.770104       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-972cs"
I0814 04:51:21.411606       1 event.go:307] "Event occurred" object="default/chatbot" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set chatbot-68b6449bc6 to 3"
I0814 04:51:21.610574       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-j77jh"
I0814 04:51:21.810885       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-rf6w7"
I0814 04:51:21.810999       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-mcm4j"
I0814 04:58:08.273273       1 event.go:307] "Event occurred" object="default/chatbot" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set chatbot-68b6449bc6 to 3"
I0814 04:58:09.195195       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-8df98"
I0814 04:58:09.673179       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-lz44p"
I0814 04:58:09.673531       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-9sdl5"
I0814 05:00:23.325696       1 event.go:307] "Event occurred" object="default/chatbot" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set chatbot-68b6449bc6 to 3"
I0814 05:00:23.452876       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-4dn2p"
I0814 05:00:23.826029       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-87ndt"
I0814 05:00:23.827419       1 event.go:307] "Event occurred" object="default/chatbot-68b6449bc6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: chatbot-68b6449bc6-4bvp4"
I0814 05:39:16.413216       1 cleaner.go:172] Cleaning CSR "csr-vnxs7" as it is more than 1h0m0s old and approved.
W0814 14:48:20.728352       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0814 14:48:20.805229       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0816 02:57:43.044914       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0816 02:57:43.044914       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
E0816 05:46:52.341173       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0816 05:46:52.341173       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
W0816 06:51:52.443266       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0816 06:51:52.443557       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
E0816 07:53:22.952809       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0816 07:53:22.952819       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0816 08:55:52.136073       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0816 08:55:52.136073       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0816 11:47:55.548055       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0816 11:47:55.548201       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0816 22:24:56.125628       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0816 22:24:56.125890       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0817 01:02:31.961672       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0817 01:02:31.961884       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
W0817 02:45:02.668117       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0817 02:45:02.668122       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
E0817 03:58:34.057541       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0817 03:58:34.057611       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0817 05:00:04.052676       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0817 05:00:04.053832       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
W0817 07:45:36.781668       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0817 07:45:36.788771       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0817 08:51:37.394951       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0817 08:51:37.394997       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0817 09:53:08.089004       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0817 09:53:08.089079       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
E0817 10:54:38.138370       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0817 10:54:38.138413       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
W0817 22:28:39.971035       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0817 22:28:39.971431       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0818 01:51:43.626457       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
E0818 01:51:43.626657       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials

* 
* ==> kube-proxy [d19a932fb99c] <==
* I0814 03:39:43.898324       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0814 03:39:43.898534       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0814 03:39:43.898622       1 server_others.go:554] "Using iptables proxy"
I0814 03:39:43.956217       1 server_others.go:192] "Using iptables Proxier"
I0814 03:39:43.956331       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0814 03:39:43.956353       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0814 03:39:43.956424       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0814 03:39:43.956485       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0814 03:39:43.962731       1 server.go:658] "Version info" version="v1.27.3"
I0814 03:39:43.962810       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0814 03:39:43.963852       1 config.go:97] "Starting endpoint slice config controller"
I0814 03:39:43.963948       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0814 03:39:43.963994       1 config.go:188] "Starting service config controller"
I0814 03:39:43.964039       1 config.go:315] "Starting node config controller"
I0814 03:39:43.964049       1 shared_informer.go:311] Waiting for caches to sync for service config
I0814 03:39:43.964055       1 shared_informer.go:311] Waiting for caches to sync for node config
I0814 03:39:44.064135       1 shared_informer.go:318] Caches are synced for node config
I0814 03:39:44.064227       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0814 03:39:44.065759       1 shared_informer.go:318] Caches are synced for service config
I0814 14:52:06.060215       1 trace.go:219] Trace[128716949]: "iptables ChainExists" (14-Aug-2023 14:52:00.440) (total time: 5607ms):
Trace[128716949]: [5.60791403s] [5.60791403s] END

* 
* ==> kube-proxy [f6bc377ad2e6] <==
* I0820 06:52:37.795615       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0820 06:52:37.795810       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0820 06:52:37.795881       1 server_others.go:554] "Using iptables proxy"
I0820 06:52:38.408070       1 server_others.go:192] "Using iptables Proxier"
I0820 06:52:38.408175       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0820 06:52:38.408195       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0820 06:52:38.408225       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0820 06:52:38.451827       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0820 06:52:38.472407       1 server.go:658] "Version info" version="v1.27.3"
I0820 06:52:38.472471       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0820 06:52:38.558229       1 config.go:315] "Starting node config controller"
I0820 06:52:38.558934       1 config.go:97] "Starting endpoint slice config controller"
I0820 06:52:38.559105       1 config.go:188] "Starting service config controller"
I0820 06:52:38.561496       1 shared_informer.go:311] Waiting for caches to sync for service config
I0820 06:52:38.561496       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0820 06:52:38.561551       1 shared_informer.go:311] Waiting for caches to sync for node config
I0820 06:52:38.662518       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0820 06:52:38.662568       1 shared_informer.go:318] Caches are synced for service config
I0820 06:52:38.662752       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [3a326daed703] <==
* E0820 06:51:55.469683       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.510286       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.510404       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.567444       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.567534       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.592430       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.592518       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.722251       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.722312       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.754363       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.754435       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.832751       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.832885       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.832766       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.832935       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.861285       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.861347       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.895121       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.895183       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:55.922443       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:55.922497       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:56.003873       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:56.003994       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:56.020582       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:56.020637       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:56.153767       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:56.153858       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:51:56.247319       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0820 06:51:56.247377       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0820 06:52:00.003169       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0820 06:52:00.003263       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0820 06:52:00.078118       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0820 06:52:00.078342       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0820 06:52:00.078469       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0820 06:52:00.078525       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0820 06:52:00.078374       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0820 06:52:00.078575       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0820 06:52:00.078539       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0820 06:52:00.078647       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0820 06:52:00.078666       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0820 06:52:00.078518       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0820 06:52:00.078903       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0820 06:52:00.078841       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0820 06:52:00.078917       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0820 06:52:00.078935       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0820 06:52:00.078946       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0820 06:52:00.078876       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0820 06:52:00.078765       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0820 06:52:00.079718       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0820 06:52:00.079068       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0820 06:52:00.079880       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0820 06:52:00.079068       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0820 06:52:00.080126       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0820 06:52:00.079227       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0820 06:52:00.080213       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0820 06:52:00.079419       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0820 06:52:00.080285       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0820 06:52:00.079618       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0820 06:52:00.080336       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
I0820 06:52:04.043820       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [3ba489f47f41] <==
* E0814 03:38:54.042757       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0814 03:38:54.330391       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0814 03:38:54.330430       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0814 03:38:54.467699       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0814 03:38:54.467841       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0814 03:38:56.493624       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0814 03:38:56.493671       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0814 03:38:57.272463       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0814 03:38:57.272518       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0814 03:38:57.280289       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0814 03:38:57.280335       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0814 03:38:57.685852       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0814 03:38:57.685898       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0814 03:38:58.251440       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0814 03:38:58.251507       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0814 03:38:58.275150       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0814 03:38:58.275216       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0814 03:38:58.381352       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0814 03:38:58.381398       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0814 03:38:58.420683       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0814 03:38:58.420728       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0814 03:38:58.666711       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0814 03:38:58.666779       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0814 03:38:58.740489       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0814 03:38:58.740540       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0814 03:38:58.837773       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0814 03:38:58.837864       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0814 03:38:58.966394       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0814 03:38:58.966453       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0814 03:38:58.982120       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0814 03:38:58.982194       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0814 03:38:59.168889       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0814 03:38:59.168939       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0814 03:38:59.253683       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0814 03:38:59.253739       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0814 03:39:05.315795       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0814 03:39:05.315851       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0814 03:39:05.857511       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0814 03:39:05.857558       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0814 03:39:05.882452       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0814 03:39:05.882527       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0814 03:39:06.673868       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0814 03:39:06.673916       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0814 03:39:06.859649       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0814 03:39:06.859710       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0814 03:39:06.924695       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0814 03:39:06.924763       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0814 03:39:06.935646       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0814 03:39:06.935718       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0814 03:39:07.022141       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0814 03:39:07.022201       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0814 03:39:07.376610       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0814 03:39:07.376703       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0814 03:39:07.404403       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0814 03:39:07.404459       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0814 03:39:07.823949       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0814 03:39:07.824009       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0814 03:39:08.204266       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0814 03:39:08.204342       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
I0814 03:39:25.305503       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Aug 20 06:52:15 minikube kubelet[1550]: E0820 06:52:15.119693    1550 kuberuntime_manager.go:1212] container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lhgxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod storage-provisioner_kube-system(e9cebdee-b9ba-4a0d-8bc2-9466a1524b54): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Aug 20 06:52:15 minikube kubelet[1550]: E0820 06:52:15.121106    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID=e9cebdee-b9ba-4a0d-8bc2-9466a1524b54
Aug 20 06:52:15 minikube kubelet[1550]: I0820 06:52:15.122471    1550 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b5a934641afe6fc6030157ea80171ab01af72df5caae84c028b7fb849cce0473"
Aug 20 06:52:15 minikube kubelet[1550]: I0820 06:52:15.136928    1550 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="dfdfedf6382019ad095055c1d565adc175010388b47a04f9ba2ca49033e32e34"
Aug 20 06:52:15 minikube kubelet[1550]: I0820 06:52:15.137327    1550 scope.go:115] "RemoveContainer" containerID="d19a932fb99cb0b50e68f16b3f02f9c53738a56def781ee28c0f70d2b85249bd"
Aug 20 06:52:15 minikube kubelet[1550]: E0820 06:52:15.140163    1550 kuberuntime_manager.go:1212] container &Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.27.3,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-w5gd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod kube-proxy-972cs_kube-system(4841a6b1-0335-4d63-85e4-d15fd34868a7): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Aug 20 06:52:15 minikube kubelet[1550]: E0820 06:52:15.140264    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-972cs" podUID=4841a6b1-0335-4d63-85e4-d15fd34868a7
Aug 20 06:52:16 minikube kubelet[1550]: I0820 06:52:16.158798    1550 scope.go:115] "RemoveContainer" containerID="75276d2616fbc353fb847a6c05699b02b4e536277e1fa5fea4757864a8aaff95"
Aug 20 06:52:16 minikube kubelet[1550]: E0820 06:52:16.180477    1550 kuberuntime_manager.go:1212] container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lhgxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod storage-provisioner_kube-system(e9cebdee-b9ba-4a0d-8bc2-9466a1524b54): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Aug 20 06:52:16 minikube kubelet[1550]: E0820 06:52:16.180580    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID=e9cebdee-b9ba-4a0d-8bc2-9466a1524b54
Aug 20 06:52:16 minikube kubelet[1550]: I0820 06:52:16.285696    1550 scope.go:115] "RemoveContainer" containerID="d19a932fb99cb0b50e68f16b3f02f9c53738a56def781ee28c0f70d2b85249bd"
Aug 20 06:52:16 minikube kubelet[1550]: E0820 06:52:16.292839    1550 kuberuntime_manager.go:1212] container &Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.27.3,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-w5gd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod kube-proxy-972cs_kube-system(4841a6b1-0335-4d63-85e4-d15fd34868a7): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Aug 20 06:52:16 minikube kubelet[1550]: E0820 06:52:16.292987    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-972cs" podUID=4841a6b1-0335-4d63-85e4-d15fd34868a7
Aug 20 06:52:17 minikube kubelet[1550]: E0820 06:52:17.338746    1550 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 20 06:52:17 minikube kubelet[1550]: E0820 06:52:17.338860    1550 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 20 06:52:18 minikube kubelet[1550]: I0820 06:52:18.413888    1550 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="652c1172c498cb308085c9858c36466bf987e2c975104880fa9b7a10ec008ed1"
Aug 20 06:52:18 minikube kubelet[1550]: I0820 06:52:18.491805    1550 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="47369990d3fc65185c3fccb3d640b46f01d2d74278c5cde2726d9c654ecf66c6"
Aug 20 06:52:18 minikube kubelet[1550]: I0820 06:52:18.560809    1550 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e1555ca91706ce6c62e3ffc691e27942e8d98ef47d6daf3e16d9a448ba1ad9e0"
Aug 20 06:52:18 minikube kubelet[1550]: I0820 06:52:18.599879    1550 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e980a6475fe9af671e56131385e433f57136234d3e180d333bc9e1656af36eb3"
Aug 20 06:52:20 minikube kubelet[1550]: I0820 06:52:20.389808    1550 scope.go:115] "RemoveContainer" containerID="75276d2616fbc353fb847a6c05699b02b4e536277e1fa5fea4757864a8aaff95"
Aug 20 06:52:27 minikube kubelet[1550]: I0820 06:52:27.753656    1550 scope.go:115] "RemoveContainer" containerID="d19a932fb99cb0b50e68f16b3f02f9c53738a56def781ee28c0f70d2b85249bd"
Aug 20 06:52:31 minikube kubelet[1550]: E0820 06:52:31.285778    1550 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 20 06:52:31 minikube kubelet[1550]: E0820 06:52:31.285906    1550 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 20 06:52:57 minikube kubelet[1550]: I0820 06:52:57.080564    1550 scope.go:115] "RemoveContainer" containerID="75276d2616fbc353fb847a6c05699b02b4e536277e1fa5fea4757864a8aaff95"
Aug 20 06:52:57 minikube kubelet[1550]: I0820 06:52:57.081007    1550 scope.go:115] "RemoveContainer" containerID="b5ef6d023aa0488f81c0e4459289079d1a40f71a4d1d432aba1f67fc0aae1cfa"
Aug 20 06:52:57 minikube kubelet[1550]: E0820 06:52:57.081291    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(e9cebdee-b9ba-4a0d-8bc2-9466a1524b54)\"" pod="kube-system/storage-provisioner" podUID=e9cebdee-b9ba-4a0d-8bc2-9466a1524b54
Aug 20 06:53:03 minikube kubelet[1550]: I0820 06:53:03.243435    1550 scope.go:115] "RemoveContainer" containerID="5f33e82ae6011e8490e089536d4b6734713ef092ab4abb90980989d81149d08d"
Aug 20 06:53:03 minikube kubelet[1550]: I0820 06:53:03.243803    1550 scope.go:115] "RemoveContainer" containerID="fd189d07b74631f694ef59ef521dc1ad07053598a414d3d8b7acfcdf5b14adb9"
Aug 20 06:53:03 minikube kubelet[1550]: E0820 06:53:03.244089    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"chatbot\" with CrashLoopBackOff: \"back-off 10s restarting failed container=chatbot pod=chatbot-68b6449bc6-87ndt_default(96432cf6-ccac-465d-b34f-a6609b0c4d3b)\"" pod="default/chatbot-68b6449bc6-87ndt" podUID=96432cf6-ccac-465d-b34f-a6609b0c4d3b
Aug 20 06:53:03 minikube kubelet[1550]: I0820 06:53:03.264103    1550 scope.go:115] "RemoveContainer" containerID="6c5c3a42c3a0b24e65bbb84ae7da2c2d534fd5565777dc7fc8024dd3ef5d42d2"
Aug 20 06:53:03 minikube kubelet[1550]: E0820 06:53:03.264372    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"chatbot\" with CrashLoopBackOff: \"back-off 10s restarting failed container=chatbot pod=chatbot-68b6449bc6-4dn2p_default(d945e2fa-1ac8-433e-b33b-3c2c4b3e547a)\"" pod="default/chatbot-68b6449bc6-4dn2p" podUID=d945e2fa-1ac8-433e-b33b-3c2c4b3e547a
Aug 20 06:53:03 minikube kubelet[1550]: I0820 06:53:03.280691    1550 scope.go:115] "RemoveContainer" containerID="cf24d569107485350fd6c1b6c0f729bd84b5c7a701cfbee38ea14ab09af84e88"
Aug 20 06:53:03 minikube kubelet[1550]: E0820 06:53:03.280913    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"chatbot\" with CrashLoopBackOff: \"back-off 10s restarting failed container=chatbot pod=chatbot-68b6449bc6-4bvp4_default(7bbba885-b7ab-4d15-b509-4dc13cd5afa9)\"" pod="default/chatbot-68b6449bc6-4bvp4" podUID=7bbba885-b7ab-4d15-b509-4dc13cd5afa9
Aug 20 06:53:03 minikube kubelet[1550]: I0820 06:53:03.878209    1550 scope.go:115] "RemoveContainer" containerID="286de9213b03a92a63c4b3cc02472e1e8570f4f1a99804f188da0b5201a39895"
Aug 20 06:53:04 minikube kubelet[1550]: I0820 06:53:04.236557    1550 scope.go:115] "RemoveContainer" containerID="7bc276655d3a666b159b013ff2eed8f8c2ff89e2613bf186d19b133ba8938d16"
Aug 20 06:53:08 minikube kubelet[1550]: I0820 06:53:08.754481    1550 scope.go:115] "RemoveContainer" containerID="b5ef6d023aa0488f81c0e4459289079d1a40f71a4d1d432aba1f67fc0aae1cfa"
Aug 20 06:53:08 minikube kubelet[1550]: E0820 06:53:08.754842    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(e9cebdee-b9ba-4a0d-8bc2-9466a1524b54)\"" pod="kube-system/storage-provisioner" podUID=e9cebdee-b9ba-4a0d-8bc2-9466a1524b54
Aug 20 06:53:16 minikube kubelet[1550]: I0820 06:53:16.755282    1550 scope.go:115] "RemoveContainer" containerID="cf24d569107485350fd6c1b6c0f729bd84b5c7a701cfbee38ea14ab09af84e88"
Aug 20 06:53:17 minikube kubelet[1550]: I0820 06:53:17.754564    1550 scope.go:115] "RemoveContainer" containerID="fd189d07b74631f694ef59ef521dc1ad07053598a414d3d8b7acfcdf5b14adb9"
Aug 20 06:53:18 minikube kubelet[1550]: I0820 06:53:18.753945    1550 scope.go:115] "RemoveContainer" containerID="6c5c3a42c3a0b24e65bbb84ae7da2c2d534fd5565777dc7fc8024dd3ef5d42d2"
Aug 20 06:53:22 minikube kubelet[1550]: I0820 06:53:22.753337    1550 scope.go:115] "RemoveContainer" containerID="b5ef6d023aa0488f81c0e4459289079d1a40f71a4d1d432aba1f67fc0aae1cfa"
Aug 20 06:53:22 minikube kubelet[1550]: E0820 06:53:22.753570    1550 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(e9cebdee-b9ba-4a0d-8bc2-9466a1524b54)\"" pod="kube-system/storage-provisioner" podUID=e9cebdee-b9ba-4a0d-8bc2-9466a1524b54
Aug 20 06:53:37 minikube kubelet[1550]: I0820 06:53:37.753831    1550 scope.go:115] "RemoveContainer" containerID="b5ef6d023aa0488f81c0e4459289079d1a40f71a4d1d432aba1f67fc0aae1cfa"
Aug 20 06:53:38 minikube kubelet[1550]: E0820 06:53:38.317416    1550 remote_image.go:208] "ImageFsInfo from image service failed" err="rpc error: code = Unknown desc = lstat /var/lib/docker/image/overlay2/layerdb/mounts/b1c6146d79b058ec531da9324ce2119e06d807e8d01f104a4991dbb71c2449ec: no such file or directory"
Aug 20 06:53:38 minikube kubelet[1550]: E0820 06:53:38.317486    1550 eviction_manager.go:262] "Eviction manager: failed to get summary stats" err="failed to get imageFs stats: rpc error: code = Unknown desc = lstat /var/lib/docker/image/overlay2/layerdb/mounts/b1c6146d79b058ec531da9324ce2119e06d807e8d01f104a4991dbb71c2449ec: no such file or directory"
Aug 20 06:56:35 minikube kubelet[1550]: W0820 06:56:35.870362    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:01:35 minikube kubelet[1550]: W0820 07:01:35.815495    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:06:35 minikube kubelet[1550]: W0820 07:06:35.816188    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:11:16 minikube kubelet[1550]: I0820 07:11:16.756435    1550 topology_manager.go:212] "Topology Admit Handler"
Aug 20 07:11:16 minikube kubelet[1550]: I0820 07:11:16.950513    1550 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5ttgh\" (UniqueName: \"kubernetes.io/projected/5ad9710a-0ca9-406a-91c2-c7022ecfae95-kube-api-access-5ttgh\") pod \"nginx-depl-6b7698588c-n46d4\" (UID: \"5ad9710a-0ca9-406a-91c2-c7022ecfae95\") " pod="default/nginx-depl-6b7698588c-n46d4"
Aug 20 07:11:20 minikube kubelet[1550]: I0820 07:11:20.572684    1550 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="290e0f7639eb7332e66f7d2c371f69c728e4d8f0e493a6a478f2d13c49e04e9a"
Aug 20 07:11:35 minikube kubelet[1550]: W0820 07:11:35.815840    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:11:46 minikube kubelet[1550]: I0820 07:11:46.233884    1550 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/nginx-depl-6b7698588c-n46d4" podStartSLOduration=9.401244981 podCreationTimestamp="2023-08-20 07:11:15 +0000 UTC" firstStartedPulling="2023-08-20 07:11:20.779028963 +0000 UTC m=+1188.563144559" lastFinishedPulling="2023-08-20 07:11:42.515539278 +0000 UTC m=+1210.299654974" observedRunningTime="2023-08-20 07:11:46.137253296 +0000 UTC m=+1213.921368992" watchObservedRunningTime="2023-08-20 07:11:46.137755396 +0000 UTC m=+1213.921871092"
Aug 20 07:16:35 minikube kubelet[1550]: W0820 07:16:35.815460    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:21:35 minikube kubelet[1550]: W0820 07:21:35.816964    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:26:35 minikube kubelet[1550]: W0820 07:26:35.816892    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:31:35 minikube kubelet[1550]: W0820 07:31:35.819211    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:36:35 minikube kubelet[1550]: W0820 07:36:35.816052    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:41:35 minikube kubelet[1550]: W0820 07:41:35.816426    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Aug 20 07:46:35 minikube kubelet[1550]: W0820 07:46:35.822374    1550 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [171bf6550acd] <==
* I0820 06:53:43.325369       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0820 06:53:43.609887       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0820 06:53:43.610025       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0820 06:54:09.967613       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0820 06:54:09.968276       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f1a6b995-1884-44cb-b6c2-7ddb63323bc9!
I0820 06:54:10.041267       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"1733243c-d871-48ee-ad7a-422e7662a799", APIVersion:"v1", ResourceVersion:"143829", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f1a6b995-1884-44cb-b6c2-7ddb63323bc9 became leader
I0820 06:54:12.846885       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f1a6b995-1884-44cb-b6c2-7ddb63323bc9!

* 
* ==> storage-provisioner [b5ef6d023aa0] <==
* I0820 06:52:25.726082       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0820 06:52:56.245583       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

